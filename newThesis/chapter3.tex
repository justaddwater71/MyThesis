\chapter{Experimental Design}
This chapter documents the concepts and technical approaches used in this thesis, as well as procedural concepts for understanding the experiments of this thesis.

\section{Experimental Design Overview}
	\paragraph{Thesis Goals} The central goal of this thesis is to understand the performance of author detection methods as a function of computational requirements. This is important to understanding the effectiveness of those same author detection methods on a resource constrained device such as a mobile phone.  Size and accuracy are critical to this thesis. This is due to the restrictive nature of mobile phones and envisioned future applications of author detection (see Chapter 5).  However, the nature of these experiments allows the results to be applied to other computing platforms with limited resources such as nano-computers, mobile sensors, or as yet unimagined devices.
	\paragraph{Experimentation}To achieve the thesis goal, experimentation will be conducted in one phase: parameter evaluation.  In parameter evaluation, we evaluate the effectiveness of different combinations of classification methods, feature sets, group sizes, and smoothing/filtering to compare prediction performance against model size.  

\section{Parameter Evaluation} This phase will evaluate numerous combinations of two classification methods, five feature sets, six grouping sizes, and three grouping methods to determine the computing requirements and effectiveness of these combinations.  Each combination will be tested to ensure the generality of the results. These combinations will be tested against two separate corpora to evaluate effectiveness in different domains. Preparing for these evaluations takes several steps including determining the required combinations, organizing and compressing the feature references, preparing the training and test data, building the models, and, finally, running the prediction tests.  The results for all prediction tests will be stored in a mySQL database which will also store the resulting f-score, precision, recall, and size of model for each test for ease of subsequent analysis.

	\subsection{Creating the Testing Combinations} The classification methods we compare are naive Bayes and Support Vector machines (SVM). Naive bayes is fast and uses a relatively small amount of RAM and disk storage. SVMs, are slower, use greater RAM and disk storage, but often yield higher f-scores.  There are numerous feature sets that can be chosen.  For this thesis, 1-grams, 2-grams, 5-grams, gappy bigrams, and orthogonal Sparse bigrams will be examined.  The intuition is that 1-grams are simple and use less space, but will be less effective than larger feature sets such as gappy bigrams or 5-grams.
	\paragraph{} For this thesis, two feature reference sets will be examined for performance enhancements, a bootstrapped bag of words and the Google Web1T corpus.  Bootstrapped bag of words simply means finding all the unique types within a training set and making each type a feature in the feature set. Since the Google Web1T corpus is huge, a parameter of that feature reference which can be adjusted is the percentage of the most frequent features that might be used.  Limiting the percentage of Web1T used reduces the storage requirements, reduces the search time to find values in the vocabulary files, and allows SVM, which is limited in how many features can be assigned, to run without an array index out of bounds error.  A more in-depth discussion of the limits of SVM is covered in the paragraph titled ``Running SVM". These experiments will permute through these numerous options to determine size, precision and f-score.  The end result will be an analysis of the utility of these various approaches to author detection on a constrained device.  A graphic of the parameter combinations is given in Figure \ref{fig:parameterCombinations}.   
	
	\begin{figure}[ht!]
		\begin{center}
			\fbox{\includegraphics[width=0.5\textwidth]{Experiment_Design.pdf}}
			\caption{Parameter Combinations for Testing}
			\label{fig:parameterCombinations}
		\end{center}
	\end{figure}
	
To examine the effect of author group size on accuracy and f-score, authors were partitioned into different group sizes.  The small numbers ``5 10 25 50 75 150" given under each column heading in Figure \ref{fig:parameterCombinations} indicate that all authors will be tested in groups of 5 unique authors, 10 unique authors, 25 unique authors, 50 unique authors, 75 unique authors, and 150 unique authors using three different grouping strategies: small-to-large, small-and-large, and random. For instance, an author would appear in a single 5 author group for a group size of 5.  There would be 30 groups of group size 5, but each author would only appear in one group. In small-to-large, the authors with the smallest amount of training data are grouped together.  In small-and-large, small authors and large authors are paired together.  In the random grouping, the authors are grouped together by a pseudo-random selection.  The reasoning for these three grouping strategies is to provide insight into the effect of prolific authors versus less prolific authors.  If results are similar for an author in a class grouping of 5, the prolific writing may not impact the outcome of author detection. This is needed information to rule out that the test author detection methods simply select the most prolific author instead of the actual author.

\paragraph*{} To more rigorously explain the groupings for small-to-large and small-and-large, the following description is provided:
	
	\subparagraph*{Author and Document Definitions}
	\begin{equation} \text{let } A = \{ a_1, a_2, ... a_n \} \end{equation}%\medspace  a_i \text{ for } 1 \le i \le n \end{equation}
	
	Let each $a_i$ in $A$, have a list of documents, $D^i$ where $D^i = \{ d_1^i, d_2^2, ... d_{m_i}^i \}$ where $m_i$ is the number of documents belonging to author $a_i$.  This can be visualized as Equation \eqref{eqn:author_doc_array}.  Note, \eqref{eqn:author_doc_array} is not necessarily square.  With the varying number of documents created by each author, \eqref{eqn:author_doc_array} will not be square for the Enron E-mail Corpus or the NPS Twitter Short Message Corpus.
	
	\begin{equation}\label{eqn:author_doc_array}
	\begin{array}{lllll}
	a_1 		& a_2 		& a_3 		& ... 	& a_n		\\
	D^1 		& D^2 		& D^3 		& ... 	& D^n		\\
	d_1^1 		& d_1^2		& d_1^3		& ...	& d_1^n		\\
	d_2^1		& d_2^2		& d_2^3		& ...	& d_2^n		\\
	.		& .		& .		& .	& . 		\\
	.		& .		& .		& .	& .		\\
	.		& .		& .		& .	& .		\\
	d_{m_1}^1	& d_{m_2}^2	& d_{m_3}^3	& ...	& d_{m_n}^n	\\
	\end{array}
	\end{equation}
	
	\begin{equation} \text{Let } |d_1^x| = file \thickspace size \thickspace of \thickspace d_1^x \thickspace in \thickspace bytes\end{equation} 
	\begin{equation} \text{Then } |D^x| = |(d_1^x| + |d_2^x| + ... + |d_{m_x}^x| = \sum_{j=1}^{m_x} |d_j^x|\end{equation}
	
	Let $L \equiv$ list of all authors, $a_i$ in $A$, rank-ordered by size such that $a_p < a_q$ if $|D^p| < |D^q|$.
	
	To represent each author as ranked by size, we need an ordinal representation of $L$. Let $L = \{ l_1, l_2, ... l_n \}$ where $l_1=a_{with \thickspace smallest \thickspace size(D)}$, $l_2=a_{with \thickspace next \thickspace larger \thickspace size(D)}$, and $l_n = a_{with \thickspace largest \thickspace size(D)}$.
	
	\subparagraph{Create Small-To-Large Groups}
	To create small-to-large groups of size $y$ from $A$, first determine the number of groups, $s$.
	\begin{equation} s=\frac{n}{y}\end{equation}
	Let $g_r=$ group $r$ of size $y$.  Then each $g_r$ is formed by assigning elements $l_{((r-1)y)+1}$ through $l_{((r-1)y)+y}$.
	
	\begin{comment}
	\begin{equation}
	\begin{array}{lcllrlcllr}
	g_{r=1}	& =	& \{ &	l_1,		& l_2,		& ...	& l_{y-1},		& l_y 		& \}	\\
	g_{r=2}	& =	& \{ &	l_{y+1},	& l_{y+2},	& ...	& l_{y+(y-1)},		& l_{2y}	& \}	\\
	.	& .	&    &	.		& .		& .	& .			& .		& \\
	.	& .	&    &	.		& .		& .	& .			& .		& \\
	.	& .	&    &	.		& .		& .	& .			& .		& \\
	g_{r=s}	& =	& \{ & 	l_{(s-1)y+1},	& l_{(s-1)y+2},	& ...	& l_{(s-1)y+(y-1)},	& l_{sy} 	& \}	\\
	\end{array}
	\end{equation}
	\end{comment}
	
	\begin{equation} b_{1i} = l_{(y-i)+1} \end{equation}
	\begin{equation} b_{2i} = l_{(y-i)+2} \end{equation}
	
	\subparagraph{Create Small-And-Large Groups}
	To create small-and-large groups of size $y$ from $A$, first divide $L$ into $y$ buckets, $b_c$, of size $\frac{n}{y}$.
	
	\begin{equation}
	\begin{array}{lclllclrcllllclr}
	b_1	& =	& \{	& l_1,				& l_2,				& ...	& l_{\frac{n}{y}}	& \}	& =	& \{ 	& b_{11},	& b_{12}	& ...	& b_{1 \frac{n}{y}} 	& \}	\\
	b_2	& =	& \{ 	& l_{\frac{n}{y}+1},		& l_{\frac{n}{y}+2},		& ...	& l_{\frac{2n}{y}}	& \}	& =	& \{ 	&  b_{21},	& b_{22}	& ...	& b_{2 \frac{n}{y}}	& \}	\\
	b_3	& =	& \{	& l_{2\frac{n}{y}+1},		& l_{2\frac{n}{y}+2},		& ...	& l_{3\frac{n}{y}}	& \}	& =	& \{ 	& b_{31},	& b_{32}	& ...	& b_{3 \frac{n}{y}}	& \}	\\
	.	& .	& 	& .				& .				& .	& .			& 	& .	&	& .		& .		& . 	& .			&	\\
	.	& .	& 	& .				& .				& .	& .			& 	& .	&	& .		& .		& . 	& .			&	\\
	.	& .	& 	& .				& .				& .	& .			& 	& .	&	& .		& .		& . 	& .			&	\\
	b_y	& =	& \{ 	& l_{(y-1)\frac{n}{y}+1},	& l_{(y-1)\frac{n}{y}+2},	& ...	& l_{y\frac{n}{y}=n} 	& \}	& =	& \{  	& b_{y1},	& b_{y2}	& ...	& b_{n\frac{n}{y}=n}	& \}	\\
	
	\end{array}
	\end{equation}
	
	Then, each group, $g_r$, is formed by assigning the $r^{th}$ element from each bucket to $g_r$.
	
	\begin{equation}
	\begin{array}{lclllclr}
	g_1		& =	& \{	& b_{11},		& b_{21},		& ...	& b_{y1}		& \}	\\
	g_2		& =	& \{	& b_{12},		& b_{22},		& ...	& b_{y2}		& \}	\\
	.		& .	&	& .			& .			& .	& .			& 	\\
	.		& .	&	& .			& .			& .	& .			& 	\\
	.		& .	&	& .			& .			& .	& .			& 	\\
	g_{\frac{n}{y}}	& =	& \{	& b_{1\frac{n}{y}}	& b_{2\frac{n}{y}}	& ...	& b_{y\frac{n}{y}}	& \}	\\
	\end{array}
	\end{equation}
	
	\subsection{Organizing and Compressing Feature References} A key element to this testing is the use of the Google Web1T corpus.  This Web1T corpus is used as a representative sample of modern language used for web sites, e-mail, short message, and other electronic communications.  The hypothesis is that, by using a corpus like Web1T as the vocabulary for author detection, one can get higher accuracies and better f-scores because Web1T represents a working model of the English language that can filter out words that do not normally appear in the English language and provide count masses that a more accurate than simple Laplace smoothing. Other researchers have already worked with Web1T as a source of smoothing counts in machine learning \cite{yuret_smoothing_2008} and for spelling correction\cite{islam_real-word_2009} with some success. 
	\paragraph*{}The Web1T corpus contains billions of unique words and word combinations with a token mass of just over 1 trillion.  The size and breadth of the Web1T corpus makes it appealing as a source for smoothing in naive Bayes and a tool for creating models in SVM. However, due to the huge size of the Web1T corpus, the text files comprising the corpus must be compressed and managed for use on desktop workstations, servers, and especially mobile devices.  Managing the corpus requires determining what portions of the Web1T corpus will be used.  Using the choice of 5-grams as an example for illustration purposes, suppose only the 5-grams portion of the Web1T corpus might be used.  The 5-grams constitute 118 text files containing up to 10 million lines of text each. Each line of the Web1T 5-gram files contains white space tokenized words (making up the type) followed by a count, separated from the words by a tab.  The lines of text are organized alphabetically by token where uppercase letters are distinct from lowercase letters. Even using only one size of gram from Web1T, a reference of this size is slow and bulky for machine learning use.  Therefore, a subset of Web1T is needed, if using Web1T proves valuable at all.
		\paragraph{Sizing the Feature Reference Set} To manage the size of Web1T, a small portion of the most frequent 5-grams could be chosen -- 1\%, 2\%, 4\%, etc.  To choose which part of the reference to use (largest, smallest, random) this thesis takes advantage of Zipf's Law.  Zipf's law states that the highest frequency word occurs approximately twice as often as the next most frequent word, implying a very small useful set.  By that reasoning, a list of the types with the highest counts is needed to capture the largest use of words in a natural language corpus.  To get this count ordered list, the complete set of Web1T n-grams are recreated offline i.e. preprocessed by a computing platform that is not the mobile device.  The recreated files list each type organized by count instead of alphabetically.  If two or more types have the same count, then those types are listed alphabetically.  The types are still listed first as a group of space separated words followed by a tab and ended with a count.

		\paragraph{Three-Tiered Hashing Scheme} Even once the feature set of types to be used for classification has been determined, the smaller set of text is still too slow to process and very bulky to store.  To further compress the data, a three-tiered hashing scheme is used.  The structure of the three-tiered hashing scheme is shown in Figure \ref{fig:3tierHashStructure}. For example, the complete Web1T vocabulary for GM1 is 178MB to store, but having to conduct a string search of 178MB of text for each individual count is very slow compared to finding an integer in a hash table. To make matters worse, the complete set of text for OSB3 is 44.1GB. Managing lookups for that much text spread over 246 files would be very slow to process. The OSB3 minimum perfect hash data structure is only 311.5MB with an accompanying signature file of 1.1GB.   
		
		\begin{figure}[ht!]
			\begin{center}
				\fbox{\includegraphics[width=0.75\textwidth]{3_tier_explanation.pdf}}
				\caption{Three-Tiered Hashing Scheme Structure}
				\label{fig:3tierHashStructure}
			\end{center}
		\end{figure}
		
		The first tier is comprised of minimal perfect hash (MPH) values of the selected feature set.  The second tier of the scheme is comprised of a 64 bit hash of the original type.  This second tier's job is to reduce the probability of a false positive in the fist tier.  This issue arises because no matter what string is input to the MPH function, a valid MPH value will be produced.  The second tier's traditional hash is accessed by mapping the MPH value to the index of an array that comprises the second tier.  That array cell contains the 64 bit hash of the original text used to create the MPH value.  This makes the false positive rate for a given type $\frac{1}{2^{64}} * \frac{1}{\text{range of MPH values}}$ which is deemed a very small risk of collision in this hashing scheme.  The third tier is simply an array of long values.  The MPH value from tier 1 is used to access this array which holds the count value for a given type.  An example of converting a phrase, ``the quick brown", is shown in Figure \ref{fig:3tierHashExample}. 
		
		\begin{figure}[ht!]
			\begin{center}
				\fbox{\includegraphics[width=0.75\textwidth]{3_tier_example.pdf}}
				\caption{Three-Tiered Hashing Scheme Example: The hash value 465 is checked against the signature hash value.  If it matches, the CMPH index provides an index to the count array, giving type count.}
				\label{fig:3tierHashExample}
			\end{center}
		\end{figure}
		
		These different tiers are not contained in a single data structure.  The MPH data structure, tier 1, is contained in a file called ``keys.mph".  The array of hash values, tier 2, is contained in a file called ``signature".  The counts are contained in a Java object file call LongCountsArrayFile. The sizes of keys.mph, signature, LongCountsArrayFile and the log probability smoothing array is contained in Chapter 4. The naive Bayes experiments use all three tiers of this structure for smoothing values.  The SVM experiments only use tier 1 and tier 2 to verify that a string encountered actually belongs to the feature set.  Specifically, the SVM data file uses integer labels for each feature, not a string value.  This means each token encountered in a training or text file must be mapped to its corresponding integer value in the MPH data structure.  Since SVM requires this integer value, but not any smoothing value, SVM only uses tier 1 and tier 2 to verify that an encountered token actually belongs to the vocabulary represented by the tier1 and tier2 files, and then gets mapped to the correct integer value for that token. These hefty data files comprise the bulk of storage required on the mobile device.  Since these data files get loaded into RAM during the prediction process, the file sizes also impact RAM requirements. The impact on RAM and disk storage makes management of the size of keys.mph, signature, and LongCountsArrayFile an important aspect of the experiments.  It is possible that other methods of storing and retrieving this data, such as using mapped files on non-volatile storage, are available. However, neither the SVM nor naive Bayes tools used in this thesis have that capability.

	\paragraph*{} The ``signature" file is needed in anticipation of an environment containing a significant number of tokens not contained in the Web1T vocabulary.  The ``signature" file creates additional storage requirements.  The ``signature" file could be done away with if the expected environment does not contain a significant number of words not contained in the Web1T.  For instance, the ``signature" file is needed for author detection in the Twitter Short Message Corpus because words in Twitter evolve nearly constantly and the Web1T Corpus was built from a 2006 snapshot of the web.  It is very likely that Twitter contains a significant number of words not found in Web1T.  If we were using another corpus drawn from a more formalized source, like the Wall Street Journal, the ``signature" file could possibly be dropped.  Since the language of the Wall Street Journal is likely more regular than Twitter, the likelihood of the wrong word getting mapped to the ``keys.mph" file is lower.  However, when only a top percentage of Web1T, not the full Web1T, is used, the need for a ``signature" file increases because the likelihood of an encountered token no being in the vocabulary goes up.

		\paragraph{Choosing Artifacts for the Three-Tiered Hashing Scheme} One impact of using MPH to reduce the size of storing types is a loss of flexibility with the text artifact selection process.  Before the MPH data structure is created, the creator must determine if punctuation, capitalization, sentence boundaries, or ``unknown" words will be allowed.  The omission of each of these artifact types brings its own unique challenges. 
		\paragraph*{} One challenge is actually creating the keys.mph, signature, smoothing counts, and smoothing probabilities files for each possible combination of artifacts.  For instance, there needs to be a keys.mph file for OSB3 with punctuation, but no sentence boundaries, capitalization, or $<\text{UNK}>$ tags.  Yet another keys.mph is needed for OSB3 without punctuation, but punctuation, sentence boundaries, capitalization, and $<\text{UNK}>$ tags are allowed. Yet another keys.mph is need for OSB3 with punctuation and sentence boundaries, but without capitalization, and $<\text{UNK}>$ tags.  The combinations of artifacts goes on.  Only one combination of artifacts was used for this thesis: punctuation allowed, sentence boundaries allowed, capitalization allowed, and $<\text{UNK}>$ tags allowed.  The keys.mph and signature files were created for all 16 combinations to support future work.
		\paragraph*{}  To manage creation of these numerous combinations in a systematic way,  a binary style number scheme was adopted. In this scheme, each possible artifact is represented as a position in a 4 bit binary number.  
		\begin{itemize}
			\item ``Capitalization converted to lowercase" is represented in the $2^0$ position. Yes is a ``1".  No is a ``0".
			\item ``Punctuation excluded" is represented in the $2^1$ position. Yes is a ``1".  No is a ``0".
			\item ``$<\text{UNK}>$ tags excluded" is represented in the $2^2$ position. Yes is a ``1".  No is a ``0".
			\item ``Sentence boundaries excluded" is represented in the $2^3$ position. Yes is a ``1".  No is a ``0".
		\end{itemize}
		For example, when capitalization, punctuation, $<\text{UNK}>$ tags, and sentence boundaries are included in a keys.mph and signature file, that keys.mph and signature file are stored in a directory named ``0".  ``0" is the description because:
		\begin{itemize}
			\item ``Is Capitalization converted to lowercase?" No. ( 0 )
			\item ``Is Punctuation excluded?" No. ( 0 )
			\item ``Are $<\text{UNK}>$ tags excluded?" No. ( 0 )
			\item ``Are sentence boundaries excluded?" No. ( 0 )
		\end{itemize}
		That results in a value of 0000 (``0").  If the answers to all the above questions are "Yes", then the keys.mph and signature would be stored in a folder called "16" because the questions result in a value of 1111 (``16").  The same naming convention is used for the smoothing counts and probabilities files.
			%For each features where capital letters hold the 1 position, punctuation the 2 position, unknown word tags the 4 position, and sentence boundaries hold the 8 position.
		 The complete matrix of artifacts allowed in the MPH model is included in Figure \ref{fig:cmphMatrix}.
		
		\begin{figure}[ht!]
			\begin{center}
				\fbox{\includegraphics[width=0.5\textwidth]{cmph_matrix.pdf}}
				\caption{Matrix of CMPH Models by Artifacts Included}
				\label{fig:cmphMatrix}
			\end{center}
		\end{figure}
		
			\subparagraph{Omitting Punctuation} Omitting punctuation provides two options for dealing with the corpus: replace punctuation with ``$<\text{UNK}>$" or drop the punctuation altogether.  If punctuation is dropped, then any type containing a punctuation mark in the feature reference set must be completely ignored.  If the punctuation is replaced with $<\text{UNK}>$, then a search within the existing count structure must be conducted for a corresponding entry for $<\text{UNK}>$ and any non-punctuation words in the type.  While dropping punctuation is much simpler to implement than employing ``$<\text{UNK}>$" tags, Google did count punctuation as a word in type construction, so correlation between n-gram counts in the Web1T corpus and the trained/predicted documents is slightly affected. To maintain simplicity, the simple drop approach was used in these experiments.
			\subparagraph{Omitting Capitalization} Omitting capitalization is straightforward for construction of tier 1 and tier 2: the encountered token is converted to all lower case and a check is conducted to see if that token is already in the MPH data structure.  For tier 3, which contains the counts, the lower case versions of the word must have its count mass added with its corresponding uppercase types.  This adds complexity to the insertion process for MPH but is easily managed.  Another option would be to simply drop all types that contained capitalization, but that would remove a large count mass from the Web1T corpus.  Adding counts was the method used in this thesis to deal with omitting capitalization.
			\subparagraph{Omitting Sentence Boundaries}  Sentence boundaries are denoted in the Web1T corpus as $<\text{S}>$ and $<\backslash \text{S}>$. Dropping sentence boundaries is straightforward since there is no replacement or count mass issues to deal with.  Since the tools for locating sentence boundaries make use of their own machine learning processes, no sentence boundaries were used in these experiments.
			\subparagraph{Omitting Unknown Words} In the Web1T corpus, ``unknown" words have a specific meaning.  To be included in any corpus n-gram set, a word must have appeared as a 1-gram at least 200 times in the Google database.  By contrast, to be 2-gram, 3-gram, 4-gram, or 5-gram, that gram had to appear at least 40 times in the Google database.  This created a situation where a word would need to appear in a 2-or-higher-gram, but was not allowed into the corpus because it did not appear 200 times in the overall database.  This is not a problem, but simply an implementation policy for the Web1T corpus.  To keep the tokens in this thesis consistent with the Web1T corpus, the tokenization process has to mirror Google's policy.  Words that fall into that category are replaced with the tag $<\text{UNK}>$ in the Web1T corpus.  %Removing $<\text{UNK}>$ words from the MPH has no effect on the counts in tier3 and is a straightforward process.
		
		\paragraph{Choosing N-Grams} N-grams can be as small as a 1-gram and grow, theoretically, to any size N imaginable.  The preferred reference set for this thesis, the Web1T corpus, uses 1, 2, 3, 4, and 5-grams.  While it is tempting to test all 5 N-gram sizes available in the corpus, only three were used.  1-grams and 5-grams were chosen to represent opposite ends of the size N gram spectrum available.  2-grams were used as a strong comparison to gappy bigrams and orthogonal sparse bigrams discussed below.  Future work could focus on 3 and 4-grams to determine if there is a performance to size advantage in using those size of N-grams.  However, it is unlikely that there is a critical point between 2-grams and 5-grams that provides significant increases in accuracy and f-score with minimal increases in storage requirements, therefore future work exploring 3 and 4-grams should not be a high priority for future work.
		
		\paragraph{Gappy Bigram and Orthogonal Sparse Bigram Construction} Once the 3 tier structure is created and functional, there are still two type of features remaining to be created.  The Web1T corpus only contains standard n-grams, not gappy bigrams or orthogonal sparse bigrams.  To create these more complex types of bigrams, a rule for counting distance and a notation scheme was needed.  It was decided to use ``lesser included counts" for both the gappy bigrams and the orthogonal sparse bigrams.  This means that a (word1, word2) pair would count for osb-0, osb-1, osb2, etc.  (A comment on notation is needed here. While previous papers wrote the distance for an OSB between word1 and word2 \cite{bikel_if_2007}, this thesis wrote the OSBs with the distance after word2 for easier parsing. This means previous papers on OSBs would record ``word1 3 word2", this thesis record ``word1 word2 3".  This is simply for ease of parsing and has no effect on the actual classification results.)  The gappy bigrams and OSBs were constructed from the 2, 3, 4, and 5-grams in the Web1T Corpus.  Word pairs from a distance of 0 (a traditional bigram or an OSB-0) to a distance of 3 (an OSB-3 or the first and last word in a 5-gram) were built from the Web1T corpus.  This process only looks at the first and last words in a 3-gram, 4-gram, or 5-gram since the inner words of this gram are already captured in the 2-gram.  Using the inner 2-grams would double count 2-grams and throw off the count mass.  %The same is true for 3-grams inside of 4 and 5-grams as well as 4-grams inside of 5-grams.
	
	\paragraph{Grouping By Size} With references built and sized, an efficient structuring of the authors and documents needs to be devised.  During data file construction, the grouping and conversion processes happened simultaneously.  The grouping sets built were: small-to-large, small-and-large, and random.  
		\subparagraph{Small-To-Large} The small-to-large group matched the least prolific authors together with increasing size up to the most prolific authors.  For example, for the 5 authors in the Enron corpus with 5 total kilobytes worth of text are group together while the 5 authors with greater than 1 total megabyte of text are group together.  No author is picked more than once.  An example is shown in Figure \ref{fig:smallToLargeGrouping}.
		\begin{figure}[ht!]
			\begin{center}
				\fbox{\includegraphics[width=0.5\textwidth]{small_to_large_grouping.pdf}}
				\caption{Small-To-Large Group for Group Size 5, 25 Authors}
				\label{fig:smallToLargeGrouping}
			\end{center}
		\end{figure}
		\subparagraph{Small-And-Large} The next group, small-and-large, is created by binning the authors by size.  Then one author from each bin is picked to be group with one author from each other bin.  For example the least prolific author is paired with one author from the most prolific bin and one author from each bin in between.  In this situation, the selection from each bin is not random.  The least prolific remaining author from each bin is picked for grouping.  No author is picked more than once. An example is shown in Figure \ref{fig:smallAndLargeGrouping}.
		\begin{figure}[ht!]
			\begin{center}
				\fbox{\includegraphics[width=0.5\textwidth]{small_and_large_grouping.pdf}}
				\caption{Small-And-Large Group for Group Size 5, 25 Authors}
				\label{fig:smallAndLargeGrouping}
			\end{center}
		\end{figure}
		\subparagraph{Random} This grouping simply produces a random number in the range of available authors and places the selected author into a group until that group is full.  Then the next group is filled the same way until no authors remain.  No author is picked more than once. No author is picked more than once. An example is shown in Figure \ref{fig:randomGrouping}.
		\begin{figure}[ht!]
			\begin{center}
				\fbox{\includegraphics[width=0.5\textwidth]{random_grouping.pdf}}
				\caption{Random Group for Group Size 5, 25 Authors}
				\label{fig:randomGrouping}
			\end{center}
		\end{figure}
		\subparagraph{Group Sizes} Based on having 150 authors in the Enron Corpus, the six following group sizes were used: 5, 10, 25, 50, 75, 150.  These six group sizes coupled with the three grouping types, small-to-large, small-and-large, and random creates 18 grouping types.  Examples of these grouping types are 5 small-to-large, 5 small-and-large, 5 random, 10 small-to-large, ..., 150 small-to-large, 150 random.  Although using all 150 authors in a grouping set makes the procedure of how the 150 were grouped redundant, all three size 150 tests were conducted as a check on the experiments. If the 150 author grouping provides different results, then there may be an issue with the classifiers. Once these grouping types were constructed, there were 171 totals sets (30 sets of 5 small-to-large, 15 sets of 10 small-to-large, ..., 1 set of 150 small-to-large, 1 set of 150 random.)  Each of these sets were intended to be run through Bootstrapped SVM, Web1T SVM, Laplace Smoothed naive Bayes, and Web1T Smoothed naive Bayes.  Assuming that only one MPH model is chosen to represent Google Web1T, that results in 684 experiments.  Since there are 16 different MPH models based on the combinations of punctuation, capitalization, sentence boundaries, and unknown words, the number of experiments could rise drastically.  However, only one MPH model is be used during the experiments resulting in only 1,368 per feature type.  Using 1-grams, 2-grams, 5-grams, 3-gb, and 3-osb results in 6,840 totals experiments.  Experimenting with the other 15 MPH models is described for future work in Chapter 5.
	
		\paragraph{Data File Format}With combinations of features, artifacts, and group sizes chosen and the MPH data structures created, the actual documents must be converted into a format that can be used by the classifiers. The LibSVM file format was used since that it is the native format for libLinear, the tool used for SVM in this thesis. \cite{fan_liblinear:_2008}  The naive Bayes classifier was built specifically for this thesis and was designed to use LibSVM format for convenience. The format of the data files consisted of an integer representing the author followed by a space, followed by a number representing the MPH value, followed by a colon, followed by another number representing the count.  Each succeeding instance of a MPH value coupled with a count is separated by a space.  Each document in the corpus is represented by a single line.  Each line's mph number is in increasing order from left to right.  The data files store the word/count pairs in a sparse fashion.  This means that a zero count is not included in the data file.  Absence of a word/count pair constitutes a zero count without needlessly using up space in the file.  An example of this file format is provided in Figure \ref{fig:svmFormat}.
		\begin{figure}[ht!]
			\begin{center}
				\fbox{\includegraphics[width=.75\textwidth]{svm_format.pdf}}
				\caption{LibSVM File Format}
				\label{fig:svmFormat}
			\end{center}
		\end{figure}
	
	Without this sparse format, the data files for libLinear would be thousands of times larger.  For a Web1T vocabulary of one billion tokens, each line of each data file would have to contain one billion hash:count pairs.  Since each line represents a document, an author with a few hundred documents would have hundreds of billions of word:count pairs.  Each of these pairs uses several bytes of storage space to record the hash value, the colon, and the count value.  This would make the data files hundreds of GB to store which is an unnecessary waste of space.
	
	\paragraph{Running SVM} With the data files created, the classifiers can be applied. The chosen tool for author detection using SVM is libLinear.  libLinear was chosen for its speed compared to LibSVM.  The libLinear source code was slightly modified to allow training a model from a data set, then running prediction on a separate set without using the built-in cross validation function.  During the training phase, each author has a SVM model built for it from a training file in a directory labeled ``train".  This training file contains hash:count pairs constructed from the minimum perfect hash and signature files constructed from the Web1T vocabulary. During the prediction phase, document contained in another file are used to predict the mostly likely author.  That file is contained in a folder called ``predict". The SVM author result is printed to a result file in a directory labeled ``result".  The f-score, precision, and recall for each file is recorded in a file inside a folder labeled ``analysis".  The analysis file also contains a full confusion matrix, time of prediction, size of original file, and other statistics.  This file is finally pulled into a mySQL database for storage and calculation of precision, recall, and f-score.
	\subparagraph{} The size of the author models impacts RAM usage and disk space.  libLinear stores SVM models as an array. RAM and storage are not the only limits.  An array of integers representing token counts can be sizable, especially when token counts are long numbers (64 bits) instead of integers (32 bits). 
	\subparagraph{} Data representation here is important.  The data can be represented in a dense or sparse format.  A dense format explicitly records every feature, even if the count for that feature is zero.  A sparse format only lists features with a non-zero count.  This allows for all features not listed in the file to be understood as zero without wasting valuable storage space.  The SVM file format uses a sparse format, but the SVM internal model uses a dense format.  This has an impact on how much RAM is required to use SVM.  
	\subparagraph{} RAM and disk storage are not the only limits. By specification, arrays in Java are limited to $2^{31}-1$ entries.  This means the model cannot contain more than $2^{31}-1$ features.  Also, the model must be loaded into RAM, so the number of authors coupled with the size of the author model must be weighed against the available RAM and disk storage.
	
	\paragraph{Running Naive Bayes} The naive Bayes classifier has been specifically built for this thesis.  The classifier reads in a pre-built array of long values from a file.  There are two types of array used in these experiments: a Laplace Smoothing array and a Google Smoothing Array.The Laplace Smoothing array is comprised of an array of an equal size to it corresponding Google Smoothing array, completely filled with 1's. The second type of array is the Google Smoothing array comprised of the count values from the Web1T corpus.  Using an array to hold the smoothing values for naive Bayes has an impact on RAM usage.  There must be enough available RAM to hold the smoothing array.  This constraint is due to the implementation of naive Bayes for this thesis, but is also a constraint when weighing the time required to look up smoothing values from a file on non-volatile storage versus looking up a smoothing value in RAM.  A study of the real impact of keeping the smoothing counts in non-volatile storage versus loading the complete files into RAM could a valuable avenue for future work.To prevent having numerous copies of the smoothing array in memory (one for each author being trained) a hashmap is used to create the author models instead.  The process for training put each encountered feature type into a hashmap along with a count of $1 +  \text{the array smoothing value}$.  If that feature type is encountered, the count is simply incremented. Once all the training documents have been read and counted, the hashmaps of feature types and counts is converted into a hashmap of feature types and log of probability.
	
	\paragraph{}During the prediction process, each encountered feature is queried against the author hashmap first.  If the feature type is found in the hashmap, then the hashmap $\log{probability}$ is used.  If not, then the smoothing array containing log of probabilities is used.  The pre-computed values of log probabilities is used to cut down on processing time.  Re-computing all required log probabilities on the mobile device would required significant processing time, but would cut down on the storage requirement. 
	\paragraph*{}An example of this hashmap/array process is shown in Figure \ref{fig:predictionFlowChart}. The result of the prediction process is outputted to a file in the corresponding results directory.  Those results are then processed into a file in the corresponding analysis folder where all data is then read into a mySQL database for evaluation of precision, recall, and f-score.
		\begin{figure}[ht!]
			\begin{center}
				\fbox{\includegraphics[width=.75\textwidth]{prediction_flow_chart.pdf}}
				\caption{Naive Bayes Hashmap and Smoothing Array Flow Chart}
				\label{fig:predictionFlowChart}
			\end{center}
		\end{figure}

%\section{Experiment Design}
%	\paragraph{}The objective of the experiments in this thesis are to populate a table of key metrics regarding author detection on a mobile phone.  The keys metrics are f-score, precision, recall, size on disk, %size in RAM, CPU consumption, and time required.  These metrics are aimed at the process of predicting the author, not at the process of training for the authors.  All author detection model training will be conducted on a workstation or server, not on a mobile phone.  The experiment will use a variety of combinations of feature sets, classification methods, and user set sizes.  The specific combinations are given in ;adslkfjads;lfkjads;lfk below:
%	\begin{center} 
%		Corpora
%		\begin{itemize}
%			\item Enron E-mail Corpus
%			\item Naval Postgraduate School Twitter Corpus
%		Classification Methods
%		\begin{itemize}
%			\item naive Bayes with Laplace Smoothing
%			\item naive Bayes with Google Web1T Count Smoothing
%			\item Support Vector Model bootstrapped from training set
%			\item Support Vector Model with Google Web1T Types for dimensions
%		\end{itemize}
%		Feature Sets
%		\begin{itemize}
%			\item 1-grams (unigrams)
%			\item 2-grams (bigrams)
%			\item 5-grams
%			\item 3-gappy bigrams
%			\item 3-orthogonal sparse bigrams
%		\end{itemize}
%		User Set Size
%		\begin{itemize}
%			\item all versus all (mutlicalss
%			\begin{itemize}
%				\item 5 users
%				\item 10 users
%				\item 25 users
%				\item 50 users
%				\item 75 users
%				\item 150 users
%			\end{itemize}
%			\item one versus all
%			\begin{itemize}
%				\item 5 users
%				\item 10 users
%				\item 25 users
%				\item 50 users
%				\item 75 users
%				\item 150 users
%			\end{itemize}

%\paragraph{}The two classifiers chosen were naive Bayes and Support Vector Model.  While there are many classifier tools available, these two methods were chosen for nominally being on the opposite end of the simplicity and effectiveness scales.  For text classification tasks, naive Bayes is often less effective than SVM.  Naives Bayes is simpler to implement than SVM, so the tradeoff is simplicity versus effectiveness.  naive Bayes is much lighter on CPU usage and time than SVM.  Both naive Bayes and SVM can process training and predicting files in a sparse format, which saves on disk storage.  Naive bayes must deal with zero probabilities stemming from a lack of training data to make the Bayes equation work out to something other than zero, and, more importantly, smoothing lessens the impact of the spiky nature of discrete data.  This smoothing is intended to provide a more reliable result.  For SVM, the SVM model can be created by only accepting words found in the actual training data or the features can be created from a feature reference, such as Google Web1T.  In the case of using a feature reference, SVM simply puts a value of 0 for each feature not provided a number in the training or prediction data.
%\section{Program Design}
%	\paragraph{}Write a cmph module to transform strings of words into numeric values.  Store those values in a file.  Have this module account for sentence boundaries, punctuation, capitalization, and unknown words.
%	\paragraph{}Transform corpus documents from a set of strings to a set of cmph created values, a different document corresponding to each cmph model (punctuation, .  Store the corpora in cmph numeric form.

%\section{Data Management}

\begin{comment}
\section{Phase Two: Android Implementation}
	\paragraph{}To manage files on the mobile device, a rudimentary file manager was built with a text viewer added.  A button was also added to the File Manager to execute prediction against a document on the phone.  An Android Service was also constructed that listens for incoming SMS messages.  When an SMS Message is ``heard", it is processed for author detection.  The Service can be turned on and off using a button on the File Manager.  
	\paragraph{}To measure CPU and RAM impact caused by the author detection processing, the third party applications, and Memory Usage, was installed on the phones.  The method is to take a baseline of the phone's CPU and RAM usage with no Widgets or Applications running, the phone is attached to a recharging device, and no calls or texts are being sent.  The same phone conditions are being set for the processing tests where the only application that will run on the phone will be the SMS capture and author detection application for this thesis.  This will yield some basic metrics of author detection impact on the phone's capabilities.
\end{comment}

\section{Corpora} Two corpora are used for this thesis: the Enron E-mail Corpus and the Naval Postgraduate School (NPS) Twitter Corpus.  The aim of this thesis is to examine author detection performance using a mobile device.  Two of the most common text communications on a mobile device are e-mail and SMS (texting).  The Enron E-mail Corpus has been widely examined and has been used for author attribution in other studies.  This makes the Enron Corpus a suitable standard to measure the author detection techniques used in this thesis.  The NPS Twitter Corpus is smaller and newer than the Enron e-mail corpus, but texting is extremely popular as a communications medium.  Determining the effectiveness of author detection over this rapidly expanding text standard is important for analyzing the effectiveness of author detection on mobile devices.
	\paragraph{Enron E-mail Corpus} Each Enron e-mail was stored in a single text file within a folder labeled with the author's first initial, second initial, and last name.  Prior to processing each Enron e-mail, a systematic attempt was made to distill each e-mail down into just the author's words.  To support this distillation, the e-mail header was stripped from each e-mail.  A search was conducted throughout the remaining text to find additional e-mail headers.  These are the embedded headers caused by e-mail replies and forwards.  Also to prevent biasing the author attribution, an attempt was made to systematically detect an e-mail closing such as ``Sincerely, Dave" or ``Yours Truly, Jane".
	\paragraph{Naval Postgraduate School Twitter Short Message Corpus} All tweets from a single author were stored in a single text file.  Each tweet from that author was contained on its own line.  Each line begins with a date-time stamp with the content of the text following.  Prior to constructing the corpus, all ``re-tweets" were removed to ensure the text came from a single author, not just from a single Twitter account.
	
\section{Intended Comparison}  Once all tests are complete, performance of the different combinations of feature and classifiers will be compared for both the Enron e-mail corpus and the Twitter Corpus.  This is to allow any differences in performance against the two primary media used on mobile phones.  The completed test results should provide insight into the possibility of author detection on a mobile phone against both e-mail and short messages.




