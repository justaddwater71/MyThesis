
@article{cortes_rational_2004,
	title = {Rational Kernels: Theory and Algorithms},
	volume = {5},
	shorttitle = {Rational Kernels},
	url = {http://portal.acm.org/citation.cfm?id=1005332.1016793&coll=ACM&dl=ACM&CFID=96681423&CFTOKEN=43711541},
	abstract = {Many classification algorithms were originally designed for fixed-size vectors. Recent applications in text and speech processing and computational biology require however the analysis of variable-length sequences and more generally weighted automata. An approach widely used in statistical learning techniques such as Support Vector Machines {(SVMs)} is that of kernel methods, due to their computational efficiency in high-dimensional feature spaces. We introduce a general family of kernels based on weighted transducers or rational relations, rational kernels , that extend kernel methods to the analysis of variable-length sequences or more generally weighted automata. We show that rational kernels can be computed efficiently using a general algorithm of composition of weighted transducers and a general single-source shortest-distance algorithm. Not all rational kernels are positive definite and symmetric {(PDS),} or equivalently verify the Mercer condition, a condition that guarantees the convergence of training for discriminant classification algorithms such as {SVMs.} We present several theoretical results related to {PDS} rational kernels. We show that under some general conditions these kernels are closed under sum, product, or Kleene-closure and give a general method for constructing a {PDS} rational kernel from an arbitrary transducer defined on some non-idempotent semirings. We give the proof of several characterization results that can be used to guide the design of {PDS} rational kernels. We also show that some commonly used string kernels or similarity measures such as the edit-distance, the convolution kernels of Haussler, and some string kernels used in the context of computational biology are specific instances of rational kernels. Our results include the proof that the edit-distance over a non-trivial alphabet is not negative definite, which, to the best of our knowledge, was never stated or proved before. Rational kernels can be combined with {SVMs} to form efficient and powerful techniques for a variety of classification tasks in text and speech processing, or computational biology. We describe examples of general families of {PDS} rational kernels that are useful in many of these applications and report the result of our experiments illustrating the use of rational kernels in several difficult large-vocabulary spoken-dialog classification tasks based on deployed spoken-dialog systems. Our results show that rational kernels are easy to design and implement and lead to substantial improvements of the classification accuracy.},
	journal = {J. Mach. Learn. Res.},
	author = {Corinna Cortes and Patrick Haffner and Mehryar Mohri},
	year = {2004},
	pages = {1035--1062}
},

@book{murphy_android_2010,
	title = {Android Beyond Java},
	isbn = {9780981678047},
	publisher = {{CommonsWare,} {LLC}},
	author = {Mark Lawrence Murphy},
	month = sep,
	year = {2010}
},

@article{broder_network_2004,
	title = {Network applications of bloom filters: A survey},
	volume = {1},
	issn = {1542-7951},
	shorttitle = {Network applications of bloom filters},
	number = {4},
	journal = {Internet Mathematics},
	author = {A. Broder and M. Mitzenmacher},
	year = {2004},
	pages = {485–509}
},

@inproceedings{sculley_relaxed_2007,
	address = {Amsterdam, The Netherlands},
	title = {Relaxed online {SVMs} for spam filtering},
	isbn = {978-1-59593-597-7},
	url = {http://portal.acm.org/citation.cfm?id=1277741.1277813&coll=ACM&dl=ACM&CFID=96681423&CFTOKEN=43711541},
	doi = {10.1145/1277741.1277813},
	abstract = {Spam is a key problem in electronic communication, including large-scale email systems and the growing number of blogs. Content-based filtering is one reliable method of combating this threat in its various forms, but some academic researchers and industrial practitioners disagree on how best to filter spam. The former have advocated the use of Support Vector Machines {(SVMs)} for content-based filtering, as this machine learning methodology gives state-of-the-art performance for text classification. However, similar performance gains have yet to be demonstrated for online spam filtering. Additionally, practitioners cite the high cost of {SVMs} as reason to prefer faster (if less statistically robust) Bayesian methods. In this paper, we offer a resolution to this controversy. First, we show that online {SVMs} indeed give state-of-the-art classification performance on online spam filtering on large benchmark data sets. Second, we show that nearly equivalent performance may be achieved by a Relaxed Online {SVM} {(ROSVM)} at greatly reduced computational cost. Our results are experimentally verified on email spam, blog spam, and splog detection tasks.},
	booktitle = {Proceedings of the 30th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {D. Sculley and Gabriel M. Wachman},
	year = {2007},
	keywords = {blogs, spam filtering, splogs, support vector machines},
	pages = {415--422}
},

@misc{brants_web_2006,
	title = {{{Web} {1T} 5-gram Version 1}},
	publisher = {Linguistic Data Consortium, Philadelphia},
	author = {Thorsten Brants and Alex Franz},
	year = {2006},
	keywords = {ir, tool}
},

@inproceedings{islam_managing_2009,
	title = {Managing the Google Web {1T} 5-gram data set},
	doi = {10.1109/NLPKE.2009.5313839},
	abstract = {This paper describes how the Google Web {1T} 5-gram data set, contributed by Google Inc., can be stored so that it can be used efficiently with respect to time. We present an efficient way of accessing all the 5-grams for a specific word of interest from the stored files. We measure the maximum access and processing efficiency achievable for any word of interest. We also compare results (access time and memory requirements) on the task of accessing all the 5-grams for a list of words, on both the processed and the original organization of the data set.},
	booktitle = {Natural Language Processing and Knowledge Engineering, 2009. {NLP-KE} 2009. International Conference on},
	author = {A. Islam and D. Inkpen},
	year = {2009},
	keywords = {data handling, Google Incorporated, Google Web {1T} 5-gram data set, Internet, natural language processing},
	pages = {1--5}
},

@book{amati_advances_2007,
	address = {Berlin, Heidelberg},
	title = {Advances in Information Retrieval},
	volume = {4425},
	isbn = {978-3-540-71494-1},
	url = {http://www.springerlink.com.libproxy.nps.edu/content/q1513k3778644204/},
	publisher = {Springer Berlin Heidelberg},
	editor = {Giambattista Amati and Claudio Carpineto and Giovanni Romano},
	year = {2007}
},

@article{klimt_introducing_????,
	title = {Introducing the Enron Corpus},
	author = {B. Klimt and Y. Yang}
},

@book{murphy_busy_2010,
	title = {The Busy Coder's Guide to Android Development},
	isbn = {9780981678009},
	publisher = {{CommonsWare}},
	author = {Mark L. Murphy},
	month = oct,
	year = {2010}
},

@inproceedings{chellapilla_gigahash:_2007,
	address = {Banff, Alberta, Canada},
	title = {{GigaHash:} scalable minimal perfect hashing for billions of urls},
	isbn = {978-1-59593-654-7},
	shorttitle = {{GigaHash}},
	url = {http://portal.acm.org/citation.cfm?id=1242572.1242747&coll=ACM&dl=ACM&CFID=99170154&CFTOKEN=68143502},
	doi = {10.1145/1242572.1242747},
	abstract = {A minimal perfect function maps a static set of n keys on to the range of integers {0,1,2,...,n - 1}. We present a scalable high performance algorithm based on random graphs for constructing minimal perfect hash functions {(MPHFs).} For a set of n keys, our algorithm outputs a description of h in expected time O(n). The evaluation of h(x) requires three memory accesses for any key x and the description of h takes up 0.89n bytes (7.13n bits). This is the best (most space efficient) known result to date. Using a simple heuristic and Huffman coding, the space requirement is further reduced to 0.79n bytes (6.86n bits). We present a high performance architecture that is easy to parallelize and scales well to very large data sets encountered in internet search applications. Experimental results on a one billion {URL} dataset obtained from Live Search crawl data, show that the proposed algorithm (a)finds an {MPHF} for one billion {URLs} in less than 4 minutes, and (b) requires only 6.86 bits/key for the description of h.},
	booktitle = {Proceedings of the 16th international conference on World Wide Web},
	publisher = {{ACM}},
	author = {Kumar Chellapilla and Anton Mityagin and Denis Charles},
	year = {2007},
	keywords = {minimal perfect hashing, perfect hash function, space efficient hash table, web search engine},
	pages = {1165--1166}
},

@inproceedings{bikel_if_2007,
	address = {Washington, {DC,} {USA}},
	title = {If We Want Your Opinion},
	isbn = {0-7695-2997-6},
	url = {http://portal.acm.org/citation.cfm?id=1304608.1306375},
	doi = {10.1109/ICSC.2007.40},
	abstract = {Sentiment has traditionally been considered a "deep" attribute of writing, often requiring the interpretation of figurative language to uncover the writer's intention. The natural language processing community has become increasingly interested in detecting, through automatic means, the expression of opinions and measuring the intensity of emotions held by the writer. Despite the depth and abstraction often associated with expressions of sentiment, we apply strictly lexical analysis to the opinions expressed about books and find that machine learning techniques are capable of resolving even fine-grained distinctions between opinions. Using an averaged perceptron classifier trained using a word subsequence kernel, we achieve an accuracy of 89\% when distinguishing between 1- and 5-star reviews. Further, this same model yields significant separation when scoring intermediate reviews--making distinctions even human annotators find difficult. We detail the collection of data for supervised training and present the results of our sentiment classifier along with some discussion about why we believe this approach to be effective.},
	booktitle = {Proceedings of the International Conference on Semantic Computing},
	publisher = {{IEEE} Computer Society},
	author = {Daniel M Bikel and Jeffrey Sorensen},
	year = {2007},
	note = {{ACM} {ID:} 1306375},
	keywords = {classifier design and evaluation, design, experimentation, language parsing and understanding, measurement, performance},
	pages = {493–500}
},

@inproceedings{estival_tat:_2007,
	title = {{TAT:} an author profiling tool with application to Arabic emails},
	shorttitle = {{TAT}},
	booktitle = {Proceedings of the Australasian Language Technology Workshop},
	author = {D. Estival and T. Gaustad and S. B Pham and W. Radford and B. Hutchinson},
	year = {2007},
	pages = {21–30}
},

@book{jurafsky_speech_2009,
	title = {Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition},
	isbn = {9780131873216},
	shorttitle = {Speech and language processing},
	publisher = {Prentice Hall},
	author = {Daniel Jurafsky and James H. Martin},
	year = {2009}
},

@inproceedings{li_support_2007,
	address = {Corvalis, Oregon},
	title = {Support cluster machine},
	isbn = {978-1-59593-793-3},
	url = {http://portal.acm.org/citation.cfm?id=1273496.1273560&coll=GUIDE&dl=GUIDE&CFID=98984342&CFTOKEN=71344517},
	doi = {10.1145/1273496.1273560},
	abstract = {For large-scale classification problems, the training samples can be clustered beforehand as a downsampling pre-process, and then only the obtained clusters are used for training. Motivated by such assumption, we proposed a classification algorithm, Support Cluster Machine {(SCM),} within the learning framework introduced by Vapnik. For the {SCM,} a compatible kernel is adopted such that a similarity measure can be handled not only between clusters in the training phase but also between a cluster and a vector in the testing phase. We also proved that the {SCM} is a general extension of the {SVM} with the {RBF} kernel. The experimental results confirm that the {SCM} is very effective for largescale classification problems due to significantly reduced computational costs for both training and testing and comparable classification accuracies. As a by-product, it provides a promising approach to dealing with privacy-preserving data mining problems.},
	booktitle = {Proceedings of the 24th international conference on Machine learning},
	publisher = {{ACM}},
	author = {Bin Li and Mingmin Chi and Jianping Fan and Xiangyang Xue},
	year = {2007},
	pages = {505--512}
},

@misc{_cmph_????,
	title = {{CMPH} - C Minimal Perfect Hashing Library},
	url = {http://cmph.sourceforge.net/},
	howpublished = {http://cmph.sourceforge.net/}
},

@article{hsu_practical_????,
	title = {A Practical Guide to Support Vector Classification},
	author = {C. W Hsu and C. C Chang and C. J Lin}
},

@incollection{schmidt_gperf:_2000,
	title = {{GPERF:} a perfect hash function generator},
	isbn = {0-521-78618-5},
	shorttitle = {{GPERF}},
	url = {http://portal.acm.org/citation.cfm?id=331120.331202&coll=GUIDE&dl=GUIDE&CFID=98945654&CFTOKEN=58295763},
	booktitle = {More C++ gems},
	publisher = {Cambridge University Press},
	author = {Douglas C. Schmidt},
	year = {2000},
	pages = {461--491}
},

@inproceedings{daoud_perfect_2007,
	address = {Lisbon, Portugal},
	title = {Perfect hash functions for large dictionaries},
	isbn = {978-1-59593-831-2},
	url = {http://portal.acm.org/citation.cfm?doid=1317353.1317368},
	doi = {10.1145/1317353.1317368},
	abstract = {We describe a new practical algorithm for finding perfect hash functions with no specification space at all, suitable for key sets ranging in size from small to very large. The method is able to find perfect hash functions for various sizes of key sets in linear time. The perfect hash functions produced are optimal in terms of time (perfect) and require at most computation of h1(k) and h2(k); two simple auxiliary pseudorandom functions.},
	booktitle = {Proceedings of the {ACM} first workshop on {CyberInfrastructure:} information management in {eScience}},
	publisher = {{ACM}},
	author = {Amjad M. Daoud},
	year = {2007},
	keywords = {acyclic, indexing, mos, perfect hashing, random graphs},
	pages = {67--72}
},

@misc{_symbian_????,
	title = {Symbian {SDKs}},
	url = {http://www.forum.nokia.com/info/sw.nokia.com/id/ec866fab-4b76-49f6-b5a5-af0631419e9c/S60_All_in_One_SDKs.html},
	howpublished = {{http://www.forum.nokia.com/info/sw.nokia.com/id/ec866fab-4b76-49f6-b5a5-af0631419e9c/S60\_All\_in\_One\_SDKs.html}}
},

@article{keselj_n-gram-based_2003,
	title = {N-gram-based author profiles for authorship attribution},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.9.7388},
	author = {Vlado Keselj and Fuchun Peng and Nick Cercone and Calvin Thomas},
	year = {2003}
},

@inproceedings{tang_email_2005,
	address = {Chicago, Illinois, {USA}},
	title = {Email data cleaning},
	isbn = {{1-59593-135-X}},
	url = {http://portal.acm.org/citation.cfm?id=1081926},
	doi = {10.1145/1081870.1081926},
	abstract = {Addressed in this paper is the issue of 'email data cleaning' for text mining. Many text mining applications need take emails as input. Email data is usually noisy and thus it is necessary to clean it before mining. Several products offer email cleaning features, however, the types of noises that can be eliminated are restricted. Despite the importance of the problem, email cleaning has received little attention in the research community. A thorough and systematic investigation on the issue is thus needed. In this paper, email cleaning is formalized as a problem of non-text filtering and text normalization. In this way, email cleaning becomes independent from any specific text mining processing. A cascaded approach is proposed, which cleans up an email in four passes including non-text filtering, paragraph normalization, sentence normalization, and word normalization. As far as we know, non-text filtering and paragraph normalization have not been investigated previously. Methods for performing the tasks on the basis of Support Vector Machines {(SVM)} have also been proposed in this paper. Features in the models have been defined. Experimental results indicate that the proposed {SVM} based methods can significantly outperform the baseline methods for email cleaning. The proposed method has been applied to term extraction, a typical text mining processing. Experimental results show that the accuracy of term extraction can be significantly improved by using the data cleaning method.},
	booktitle = {Proceedings of the eleventh {ACM} {SIGKDD} international conference on Knowledge discovery in data mining},
	publisher = {{ACM}},
	author = {Jie Tang and Hang Li and Yunbo Cao and Zhaohui Tang},
	year = {2005},
	keywords = {data cleaning, email processing, statistical learning, text mining},
	pages = {489--498}
},

@article{fan_liblinear:_2008,
	title = {{LIBLINEAR:} A Library for Large Linear Classification},
	volume = {9},
	shorttitle = {{LIBLINEAR}},
	url = {http://portal.acm.org/citation.cfm?id=1390681.1442794&coll=ACM&dl=ACM&CFID=96681423&CFTOKEN=43711541},
	abstract = {{LIBLINEAR} is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that {LIBLINEAR} is very efficient on large sparse data sets.},
	journal = {J. Mach. Learn. Res.},
	author = {{Rong-En} Fan and {Kai-Wei} Chang and {Cho-Jui} Hsieh and {Xiang-Rui} Wang and {Chih-Jen} Lin},
	year = {2008},
	pages = {1871--1874}
},

@misc{_enron_????,
	title = {Enron Email Dataset},
	url = {http://www-2.cs.cmu.edu/%7Eenron/},
	howpublished = {{http://www-2.cs.cmu.edu/\%7Eenron/}}
},

@article{bloom_space/time_1970,
	title = {Space/time trade-offs in hash coding with allowable errors},
	volume = {13},
	issn = {0001-0782},
	number = {7},
	journal = {Communications of the {ACM}},
	author = {B. H Bloom},
	year = {1970},
	pages = {422–426}
},

@misc{_creating_????,
	title = {Creating an {iPhone} Application},
	url = {http://developer.apple.com/library/ios/#referencelibrary/GettingStarted/Creating_an_iPhone_App/index.html},
	journal = {Creating an {iPhone} Application},
	howpublished = {{http://developer.apple.com/library/ios/\#referencelibrary/GettingStarted/Creating\_an\_iPhone\_App/index.html}}
},

@article{pagh_hash_1999,
	title = {Hash and displace: Efficient evaluation of minimal perfect hash functions},
	shorttitle = {Hash and displace},
	journal = {Algorithms and Data Structures},
	author = {R. Pagh},
	year = {1999},
	pages = {767–767}
},

@inproceedings{guthrie_minimal_2010,
	title = {Minimal Perfect Hash Rank: Compact Storage of Large N-gram Language Models},
	shorttitle = {Minimal Perfect Hash Rank},
	booktitle = {Web N-gram Workshop},
	author = {D. Guthrie and M. Hepple},
	year = {2010},
	pages = {29}
},

@inproceedings{yang_improved_2006,
	title = {An Improved Cascade {SVM} Training Algorithm with Crossed Feedbacks},
	isbn = {0-7695-2581-4},
	url = {http://portal.acm.org/citation.cfm?id=1136466},
	abstract = {Support Vector Machine {(SVM)} has become a popular classification tool but one of its disadvantages is large memory requirement and computation time when dealing with large datasets. Parallel methods have been proposed to speed up the process of training {SVM.} An improved cascade {SVM} training algorithm is proposed, in which multiple {SVM} classifiers are applied. The support vectors are obtained by feeding back in a crossed way, alternating to avoid the problem that the learning results are subject to the distribution state of the data samples in different subsets. The experiment results on {UCI} dataset show that this parallel {SVM} training algorithm is efficient and has more satisfying accuracy compared with standard cascade {SVM} algorithm in classification precision.},
	booktitle = {Proceedings of the First International {Multi-Symposiums} on Computer and Computational Sciences - Volume 2  {(IMSCCS'06)} - Volume 02},
	publisher = {{IEEE} Computer Society},
	author = {Jing Yang},
	year = {2006},
	pages = {735--738},
	annote = {{{\textless}p{\textgreater}Paper} is at {IEEE} vice {ACM.}  {VPN} access did not get me access to the library will try through actual {BOSUN} next.{\textless}/p{\textgreater}}
},

@misc{_streaming_????,
	title = {Streaming {API} Documentation {\textbar} dev.twitter.com},
	url = {http://dev.twitter.com/pages/streaming_api},
	journal = {Streaming {API} Documentation},
	howpublished = {http://dev.twitter.com/pages/streaming\_api}
},

@book{manning_introduction_2008,
	title = {Introduction to information retrieval},
	isbn = {9780521865715},
	publisher = {Cambridge University Press},
	author = {Christopher D. Manning and Prabhakar Raghavan and Hinrich Schütze},
	year = {2008}
},

@misc{_gartner_????,
	title = {Gartner Says Worldwide Mobile Phone Sales Grew 17 Per Cent in First Quarter 2010},
	url = {http://www.gartner.com/it/page.jsp?id=1372013},
	journal = {Gartner Says Worldwide Mobile Phone Sales Grew 17 Per Cent in First Quarter 2010},
	howpublished = {http://www.gartner.com/it/page.jsp?id=1372013}
},

@article{jenkins_hash_1997,
	title = {Hash functions},
	volume = {22},
	issn = {{1044789X}},
	abstract = {Jenkins discusses how hash functions can fail and uses this principle to design a new hash function.},
	number = {9},
	journal = {Dr. Dobb's Journal},
	author = {Bob Jenkins},
	month = sep,
	year = {1997},
	keywords = {Algorithms},
	pages = {107}
},

@misc{_method_2010,
	title = {Method and system for detection of authors},
	url = {http://www.google.com/patents?hl=en&lr=&vid=USPAT7752208&id=quXRAAAAEBAJ&oi=fnd&dq=%22author+detection%22&printsec=abstract},
	month = jul,
	year = {2010},
	note = {{undefinedFiling} Date: Apr 11, 2007}
},

@inproceedings{gauravaram_update_2007,
	address = {Chennai, India},
	title = {An update on the side channel cryptanalysis of {MACs} based on cryptographic hash functions},
	isbn = {3-540-77025-9, 978-3-540-77025-1},
	url = {http://portal.acm.org/citation.cfm?id=1777898.1777939&coll=ACM&dl=ACM&CFID=96859924&CFTOKEN=42380848},
	abstract = {Okeya has established that {HMAC/NMAC} implementations based on only {Matyas-Meyer-Oseas} {(MMO)} {PGV} scheme and his two refined {PGV} schemes are secure against side channel {DPA} attacks when the block cipher in these constructions is secure against these attacks. The significant result of Okeya's analysis is that the implementations of {HMAC/NMAC} with the {Davies-Meyer} {(DM)} compression function based hash functions such as {SHA-1} are vulnerable to {DPA} attacks. In this paper, first we show a partial key recovery attack on {NMAC/HMAC} based on Okeya's two refined {PGV} schemes by taking practical constraints into consideration. Next, we propose new hybrid {NMAC/HMAC} schemes for security against side channel attacks assuming that their underlying block cipher is ideal. We show a hybrid {NMAC/HMAC} proposal which can be instantiated with {DM} and a slight variant to it allowing {NMAC/HMAC} to use hash functions such as {SHA-1.} We then show that {M-NMAC,} {MDx-MAC} and a variant of the envelope {MAC} scheme based on {DM} with an ideal block cipher are secure against {DPA} attacks.},
	booktitle = {Proceedings of the cryptology 8th international conference on Progress in cryptology},
	publisher = {{Springer-Verlag}},
	author = {Praveen Gauravaram and Katsuyuki Okeya},
	year = {2007},
	keywords = {dpa, hmac, mdx-mac, m-nmac, side channel attacks},
	pages = {393--403}
},

@article{fisher_use_1936,
	title = {{{The} use of multiple measurements in taxonomic problems}},
	volume = {7},
	number = {2},
	journal = {Annals of Eugenics},
	author = {{RA} Fisher},
	year = {1936},
	keywords = {agent, data-mining},
	pages = {179--188}
},

@article{graf_parallel_????,
	title = {Parallel support vector machines: The cascade svm},
	shorttitle = {Parallel support vector machines},
	author = {H. P Graf and E. Cosatto and L. Bottou and V. Vapnik}
},

@inproceedings{yu_large_2010,
	title = {Large linear classification when data cannot fit in memory},
	booktitle = {Proceedings of the 16th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	author = {H. F Yu and C. J Hsieh and K. W Chang and C. J Lin},
	year = {2010},
	pages = {833–842}
},

@article{germann_tightly_????,
	title = {Tightly Packed Tries: How to Fit Large Models into Memory, and Make them Load Fast, Too},
	shorttitle = {Tightly Packed Tries},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.164.3395},
	author = {Ulrich Germann and Eric Joanis and Samuel Larkin}
},

@article{zoellick_fragile_2008,
	title = {Fragile States: Securing Development},
	volume = {50},
	issn = {0039-6338},
	shorttitle = {Fragile States},
	url = {http://www.informaworld.com/10.1080/00396330802601859},
	doi = {10.1080/00396330802601859},
	number = {6},
	journal = {Survival: Global Politics and Strategy},
	author = {Robert B. Zoellick},
	year = {2008},
	pages = {67}
},

@article{sokolova_beyond_2006,
	title = {Beyond accuracy, f-score and roc: a family of discriminant measures for performance evaluation},
	shorttitle = {Beyond accuracy, f-score and roc},
	journal = {{AI} 2006: Advances in Artificial Intelligence},
	author = {M. Sokolova and N. Japkowicz and S. Szpakowicz},
	year = {2006},
	pages = {1015–1021}
},

@article{putze_cache-_2009,
	title = {Cache-, hash-, and space-efficient bloom filters},
	volume = {14},
	url = {http://portal.acm.org/citation.cfm?id=1498698.1594230&coll=ACM&dl=ACM&CFID=99170154&CFTOKEN=68143502},
	doi = {10.1145/1498698.1594230},
	abstract = {A Bloom filter is a very compact data structure that supports approximate membership queries on a set, allowing false positives.},
	journal = {J. Exp. Algorithmics},
	author = {Felix Putze and Peter Sanders and Johannes Singler},
	year = {2009},
	keywords = {approximate dictionary, data compression},
	pages = {4.4--4.18}
},

@article{vapnik_support-vector_1995,
	title = {{Support-Vector} Networks},
	volume = {20},
	issn = {0885-6125},
	url = {http://dx.doi.org/10.1023/A:1022627411411},
	journal = {Machine Learning},
	author = {Vladimir Vapnik and Corinna Cortes},
	year = {1995},
	note = {{10.1023/A:1022627411411}},
	pages = {273--297}
},

@misc{_blackberry_????,
	title = {{BlackBerry} - {BlackBerry} Developer Zone},
	url = {http://us.blackberry.com/developers/},
	journal = {{BlackBerry} Developer Zone},
	howpublished = {http://us.blackberry.com/developers/}
},

@inproceedings{botelho_external_2007,
	address = {Lisbon, Portugal},
	title = {External perfect hashing for very large key sets},
	isbn = {978-1-59593-803-9},
	url = {http://portal.acm.org/citation.cfm?id=1321440.1321532&coll=ACM&dl=ACM&CFID=99170154&CFTOKEN=68143502},
	doi = {10.1145/1321440.1321532},
	abstract = {We present a simple and efficient external perfect hashing scheme (referred to as {EPH} algorithm) for very large static key sets. We use a number of techniques from the literature to obtain a novel scheme that is theoretically well-understood and at the same time achieves an order-of-magnitude increase in the size of the problem to be solved compared to previous "practical" methods. We demonstrate the scalability of our algorithm by constructing minimum perfect hash functions for a set of 1.024 billion {URLs} from the World Wide Web of average length 64 characters in approximately 62 minutes, using a commodity {PC.} Our scheme produces minimal perfect hash functions using approximately 3.8 bits per key. For perfect hash functions in the range {0,...,2n - 1} the space usage drops to approximately 2.7 bits per key. The main contribution is the first algorithm that has experimentally proven practicality for sets in the order of billions of keys and has time and space usage carefully analyzed without unrealistic assumptions.},
	booktitle = {Proceedings of the sixteenth {ACM} conference on Conference on information and knowledge management},
	publisher = {{ACM}},
	author = {Fabiano C. Botelho and Nivio Ziviani},
	year = {2007},
	keywords = {functions, hash, key sets, large, minimal, perfect},
	pages = {653--662}
},

@article{bordes_fast_2005,
	title = {Fast Kernel Classifiers with Online and Active Learning},
	volume = {6},
	url = {http://portal.acm.org/citation.cfm?id=1046920.1194898&coll=ACM&dl=ACM&CFID=96681423&CFTOKEN=43711541},
	abstract = {Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efficient learning algorithm should at least take a brief look at each example. But should all examples be given equal {attention?This} contribution proposes an empirical answer. We first present an online {SVM} algorithm based on this premise. {LASVM} yields competitive misclassification rates after a single pass over the training examples, outspeeding state-of-the-art {SVM} solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.},
	journal = {J. Mach. Learn. Res.},
	author = {Antoine Bordes and Seyda Ertekin and Jason Weston and Léon Bottou},
	year = {2005},
	pages = {1579--1619}
},

@book{russell_artificial_2004,
	title = {Artificial Intelligence: a Modern Approach},
	isbn = {9780137903023},
	shorttitle = {Artificial Intelligence},
	publisher = {Pearson plc},
	author = {Stuart Russell},
	month = dec,
	year = {2004}
},

@article{kramer_fast_2009,
	title = {Fast Support Vector Machines for Continuous Data},
	volume = {39},
	issn = {1083-4419},
	doi = {10.1109/TSMCB.2008.2011645},
	abstract = {Support vector machines {(SVMs)} can be trained to be very accurate classifiers and have been used in many applications. However, the training time and, to a lesser extent, prediction time of {SVMs} on very large data sets can be very long. This paper presents a fast compression method to scale up {SVMs} to large data sets. A simple bit-reduction method is applied to reduce the cardinality of the data by weighting representative examples. We then develop {SVMs} trained on the weighted data. Experiments indicate that bit-reduction {SVM} produces a significant reduction in the time required for both training and prediction with minimum loss in accuracy. It is also shown to typically be more accurate than random sampling when the data are not overcompressed.},
	number = {4},
	journal = {Systems, Man, and Cybernetics, Part B: Cybernetics, {IEEE} Transactions on},
	author = {{K.A.} Kramer and {L.O.} Hall and {D.B.} Goldgof and A. Remsen and Tong Luo},
	year = {2009},
	keywords = {data compression, data representation, data structures, fast compression method, fast support vector machines, large data sets, simple bit-reduction method, support vector machines},
	pages = {989--1001}
},

@misc{_multiclass_????,
	title = {Multiclass {SVMs}},
	url = {http://nlp.stanford.edu/IR-book/html/htmledition/multiclass-svms-1.html},
	howpublished = {{http://nlp.stanford.edu/IR-book/html/htmledition/multiclass-svms-1.html}}
},

@incollection{belazzougui_hash_2009,
	title = {Hash, Displace, and Compress},
	url = {http://dx.doi.org/10.1007/978-3-642-04128-0_61},
	abstract = {A hash function h, i.e., a function from the set U of all keys to the range range [m] = {0,...,m − 1} is called a perfect hash function {(PHF)} for a subset S ⊆ U of size n ≤ m if h is 1-1 on S. The important performance parameters of a {PHF} are representation size, evaluation time and construction time. In this paper,
we present an algorithm that permits to obtain {PHFs} with expected representation size very close to optimal while retaining
O(n) expected construction time and O(1) evaluation time in the worst case. For example in the case m = 1.23n we obtain a {PHF} that uses space 1.4 bits per key, and for m = 1.01n we obtain space 1.98 bits per key, which was not achievable with previously known methods. Our algorithm is inspired by several
known algorithms; the main new feature is that we combine a modification of Pagh’s “hash-and-displace” approach with data
compression on a sequence of hash function indices. Our algorithm can also be used for k-perfect hashing, where at most k keys may be mapped to the same value.},
	booktitle = {Algorithms - {ESA} 2009},
	author = {Djamal Belazzougui and Fabiano Botelho and Martin Dietzfelbinger},
	year = {2009},
	pages = {682--693}
},

@book{alpaydin_introduction_2004,
	title = {Introduction to machine learning},
	isbn = {9780262012119},
	publisher = {{MIT} Press},
	author = {Ethem Alpaydin},
	month = oct,
	year = {2004}
},

@misc{_smoothing_????,
	title = {Smoothing a tera-word language model},
	url = {http://portal.acm.org/citation.cfm?id=1557727&dl=},
	howpublished = {http://portal.acm.org/citation.cfm?id=1557727\&dl=}
},

@inproceedings{crammer_advanced_2008,
	address = {Columbus, Ohio},
	title = {Advanced online learning for natural language processing},
	url = {http://portal.acm.org/citation.cfm?id=1564169.1564173&coll=ACM&dl=ACM&CFID=96681423&CFTOKEN=43711541},
	abstract = {Most research in machine learning has been focused on binary classification, in which the learned classifier outputs one of two possible answers. Important fundamental questions can be analyzed in terms of binary classification, but real-world natural language processing problems often involve richer output spaces. In this tutorial, we will focus on classifiers with a large number of possible outputs with interesting structure. Notable examples include information retrieval, part-of-speech tagging, {NP} chucking, parsing, entity extraction, and phoneme recognition.},
	booktitle = {Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Tutorial Abstracts},
	publisher = {Association for Computational Linguistics},
	author = {Koby Crammer},
	year = {2008},
	pages = {4--4}
},

@book{love_attributing_2002,
	title = {Attributing authorship: an introduction},
	isbn = {9780521789486},
	shorttitle = {Attributing authorship},
	publisher = {Cambridge University Press},
	author = {Harold Love},
	month = jun,
	year = {2002}
}