\chapter{Conclusions and Future Work}

\section{Summary}
\paragraph*{} This thesis asked one basic question: can author detection be accomplished on a mobile device?  To answer that question, several supporting questions had to be answered: 
\begin{itemize}
	\item For the two dominant mobile phone text mediums, short message and e-mail, what combination of classification method and feature type provides the best accuracy?
	\item What is the storage requirement for each combination of method and feature type?
	\item What is the relative value of classification accuracy versus storage requirement for each classification method and feature type?
	\item What is the impact on performance due to number of distinct authors (group size)?
	\item Does the relative prolificity of each author in a detection group significantly affect the accuracy of each classification method and feature type?
	\item Does a highly effective method-feature type combination exist with a small enough storage requirement to be executed on a mobile device?
\end{itemize}

	\paragraph*{} Two classification methods, naive Bayes and SVM, were tested against five feature types: 1-grams (GM1), 2-grams (GM2), 5-grams (GM5), gappy bigrams of distance 3 (GB3), and orthogonal sparse bigrams of distance 3 (OSB3).  For each of these combinations, six vocabularies were used.  Five of the vocabularies were drawn from a specific percentage of the highest count features found in the Google Web1T Corpus.  Specifically, the top 1\%, 2\%, 4\%, 8\%, and 16\% of Google Web1T were used as vocabularies.  The sixth vocabulary was a ``bootstrapped" vocabulary that was drawn directly from the training corpus with no reference to an outside set of features.  This vocabulary was represented as Web1T\% 0 since the bootstrapped vocabulary used 0\% of the Google Web1T Corpus as a vocabulary reference.

	\paragraph*{} Testing was conducted using a 80/20 cross validation against each set of selected authors.  The testing of two classification methods, five feature types, three grouping methods, and six vocabulary combinations over two corpora, Enron and Twitter, resulted in 19,782 tests producing 286,050 measurements and 19,782 measurements for average accuracy.  Analysis of these results finds:
\begin{itemize} 
	\item The method-feature type combination that suited mobile devices best for the Enron E-mail Corpus for all author group sizes was Support Vector Machine classification using 1-grams as a feature type and no reference to the Google Web1T Corpus for vocabulary.  This combination produced an average accuracy of 77.4\%  with a standard deviation of 12.2\% and average f-score of .6257 requiring 4.83MB (i.e. a feasible amount) of storage on the device.
	\item The method-feature type combination that suited mobile devices best for the Twitter Short Message Corpus was Support Vector Machine using Gappy Bigrams with a word distance of 3 and no reference to the Google Web1T Corpus for vocabulary. This combination produced an average accuracy of 62.2\% with a standard deviation of 7.9\% and average f-score of 0.4820 requiring 3.59MB of storage on the device.
	\item Very prolific authors were detected with greater accuracy and f-score than less prolific authors, even when a prolific author was grouped with other prolific authors.
	\item Author detection accuracy and f-score, in the Enron E-mail Corpus was significantly higher than in the Twitter Short Message Corpus. However, it was not clear from the results if this disparity in accuracy is due to language differences between e-mail and short message or due to having a large amount of e-mail text compared to the amount of short message text.
	\item Similarly prolific authors had lower accuracies, but higher f-scores than dissimilarly prolific authors. We explain this phenomenon in detail in Chapter 4, Section 4.2.
	\item Storage requirements for many of the model-feature combinations were too large for use on an existing mobile device.  The most powerful method-feature combinations often had storage requirements above 2GB.  If memory were not a constraint, we find that the best performing method-feature combination is SVM using OSB3 and Web1T\% of 0 in a group size of 5 with an average accuracy of 86.9\% with a standard deviation of 9.8\% and an f-score of 0.68\%.  For Twitter, the best method-feature combination without regard for memory is naive Bayes using OSB3 for a group size of 5 with an average accuracy of 64.7\% with a standard deviation of 6.7\% and an f-score of 0.63.
	\item There is a small number of method-feature combinations that can meet the storage limitations of a mobile device and still produce accuracies higher than the Maximum Likelihood Estimate (MLE) for author detection.  Whether these accuracies are sufficiently high for practical application is left for future work.
\end{itemize}
	
	\paragraph*{} These results show that author detection on a mobile device can be implemented and test knowing that storage requirement is only an indication of actual memory required.  An implementable author detection capability is worthy of future work.

\section{Future Work}

This thesis sets the stage for several future work efforts.  Some future work should focus specifically on implementation of mobile device author detection tools.  Other efforts could focus on the natural language processing impacts of the model-feature combinations. Specific items for future work are:
\begin{itemize}
	\item Implementation of Author Detection in an Operational Environment
	\item Explore Google Web1T as a Tool for Natural Language Processing
	\item Continue Experiments including varied Minimum Perfect Hash models and varied classification techniques
	\item Conduct Further Statistical Analysis of Gathered Data
\end{itemize}

\begin{singlespace}
\section{Implementation of Author Detection in an Operational Environment}
\end{singlespace}

	\begin{singlespace}
	\subsection{Test the Top Scoring Method-Feature Combinations on Android Phones and other Java-Capable Mobile Platforms}
	\end{singlespace}
		\paragraph*{} As part of the preparation for this thesis, an Android application was written to record SMS messages as they were received on an Android phone.  This application had a rudimentary file browser to manage the captured SMS messages.  The tools for this thesis were all written in Java to make the transition to Android phones seamless.  Even the libLinear version used was written in Java with an expectation toward deployment on Android phone.
		\paragraph*{} With these Android preparations and the results of this thesis, the only limiting factor to running author detection tests on a variety of Android phones is time.  Another researcher with access to a small number of Android phones or tablets could conduct these tests in a few weeks.  The tests should span the range of Android versions from Android version 1.6 all the way up to Android version 3.0.  Underpowered phones such as the HTC ADP2 should be tested along with higher end phones like the Google ADP3 or the Motorola Xoom.  These tests could determine not only the feasibility of author detection on a mobile device, but the impact on CPU usage, battery life, and SDcard life.

	\subsection{Determine Appropriate Group Size for Target Authors}
	\paragraph*{}  Training to a large number of authors may not be necessary.  Do most mobile device users have a social network of 150 people they routinely communicate with via mobile phone?  While a mobile device user may have hundreds of ``friends" on Facebook, they may only send text messages and e-mails to a small number of people.  If a reasonable number can be determined for expected authors to be detected, then the choice of method-feature combination can be specifically refined to that number of people.  This would also keep the authors model size minimized for disk storage.


	\subsection{Study ``Stealthiness" for Author Detection Software}
	\paragraph*{} For a covert delivery of author detection tools, it is important to keep the presence of author detection tools unnoticed by the user.  If the author detection tool causes lag in the user interface, noticeably reduces battery time, consumes a large portion of storage, or increases a user's wireless bill, that tool will get noticed.
	\paragraph*{} Methods to conceal the author detection tools could be as simple as storing e-mail and short messages throughout the day, then processing them only when the device is on a charger during night hours.  The covert mechanism could be very sophisticated and learn user patterns to find an optimal time to process.  The covert portion of the tools should cease operation if the user picks up the phone or receives a call.
	\paragraph*{} Even if the act of author detection is concealed, the "author-detected" method of informing an outside facility must avoid detection as well.  Sending an SMS could attract attention.  Creating a data connection over wireless unexpectedly could draw attention as well.  Using some covert channel to alert an outside facility would be an important part of using these author detection tools in a covert delivery environment.

	\subsection{Study Disk Storage to RAM Usage for Mobile Phones}
	\paragraph*{} The storage requirement measurement in this thesis focused heavily on the standard Dalvik VM limit of 16MB.  To reduce the impact of large vocabularies on the Dalvik VM, the vocabulary could be read directly from non-volatile storage such as a microSD card.  This would slow processing of intercepted e-mails or short messages, but could greatly expand the number of mobile device appropriate method-feature combinations.
	\paragraph*{} Studying techniques of accessing vocabulary files direct from disk would entail more than just developing a random access file to hold the vocabulary objects.  The impact on processing time, stealthiness, and possible interruption of applications on the mobile device would need to be examined.  Also, the behavior of Liblinear and naive Bayes (as developed for this thesis) would need to be changed to handle a random access file instead of directly accessing RAM.
	\paragraph*{} Storage requirements were measured using on-disk sizes for the author detection tools.  Developing a deterministic model of on-disk storage requirements to actual volatile memory requirements would support choosing appropriate method-feature combinations for testing on mobile devices.  Such a deterministic model would prevent testing method-feature combinations that are clearly too large for a Dalvik VM.

\begin{singlespace}
\subsection{Conduct LibLinear and Naive Bayes Tests With a Large ``Noise" Group}
\end{singlespace}
\paragraph*{} One of the most important tests that could be run against the data in this thesis would be to create ``noise groups" to test the accuracy and f-score of the author detection tools in this thesis.  To do this, a 150-author group could have all but 5 authors relabeled as ``author X".  This would create a six author test set where one author is actually a mix of 145 authors (``author X").  Such a test would provide an indication as to how well these tools work in an environment filled with many non-targets authors and a few target authors.
\paragraph*{} This same test could be conducted with 10 authors and an ``author X", 25 authors with an ``author X", etc.  This is a more realistic test scenario than the 5 versus 5, 10 versus 10, etc tests conducted in this thesis.  
\paragraph*{} Such tests would likely be very time consuming.  150-versus-150 tests in this thesis often took hours to execute. However, having a large number of documents to process against only six authors would reduce the overall processing time.  Though time consuming, the noise group tests are an important next step towards implementation.

\subsection{Test Spoken Keyword Recognition Techniques} 
\paragraph*{}With text processing examined for use on mobile devices, a natural progression is to detect key words on a mobile device.  Conducting author detection using voice recognition is likely beyond the capability of 2011 mobile devices. However, detecting key words or combinations of key words using voice techniques may not be feasible.
\paragraph*{} For example, detecting words often associated with an attack on a convoy could be incorporated on a mobile device which then sends a signal to a central alert center to warn nearby convoys.  A teenager's phone could recognize key words associated with drug use or other dangerous behaviors.  Parents could then receive an alert.
\paragraph*{} Voice processing is much more difficult than text processing and would require a substantially different approach from this thesis.  Also, accounting for voice tenor and variation in phonemes between languages would be complex. In the end, the operations task of creating author text models may be more daunting than the complexity of keyword recognition from phonemes, thus making keyword recognition a viable path of research.




\begin{singlespace}
\section{Explore Google Web1T as a Tool for Natural Language Processing}
\end{singlespace}

	\begin{singlespace}
	\subsection{Determine Accuracy and F-Score for Other Web1T Vocabulary Variations}
	\end{singlespace}
		\paragraph*{} To support this thesis, minimum perfect hash data structure files and hash signature files were created for every permutation of allowing punctuation, capitalization, sentence boundaries, and handling the Web1T ``unknown word" tag.  Only one permutation, which allows punctuation, capitalization, sentence boundaries, and the Web1T ``unknown word" tag was used in this thesis.  The remaining 15 permutations could also be tested to determine their accuracy, f-score, and storage requirements.
		\paragraph*{} Handling punctuation, capitalization, and the Web1T ``unknown word" tag is already coded into the Java code used for this thesis.  Sentence boundaries are more difficult to deal with.  Tools that use maximum entropy to find sentence boundaries are available, but were deemed too computationally expensive to use in the already process-intensive and memory-intensive environment of author detection.  An efficient means of sentence boundary detection would need to be found before making actual use of sentence boundaries.  To that end, permutation ``8" (punctuation allowed, capitalization allowed, ``unknown word" tag allowed, sentence boundaries not allowed) should give identical results to this thesis. The reason for identical results is this thesis allowed for sentence boundaries in the Web1T vocabulary, but had no mechanism to train to sentence boundaries.

	\subsection{Test 3-Grams and 4-Grams}
	\paragraph*{}We did not test 3-grams and 4-grams.  3-grams and 4-grams were hypothesized to have accuracies and storage requirements between 2-grams and 5-grams.  It is unlikely that 3-grams and 4-grams would show significantly higher accuracy than 2-grams or 5-grams.  It is easily predictable that 3-grams and 4-grams would have a storage requirement greater than 2-grams, but less than 5-grams.

\begin{singlespace}
\section{Continue Experiments in This Thesis}
\end{singlespace}

	\subsection{Rewrite LibLinear Data Structures}
		\paragraph*{} A limiting factor in testing method-feature combinations with libLinear was the maximum number of elements the libLinear model was able to hold.  Since the core of the libLinear model is an array of float values, that array is limited to $2^{31}$ elements.  When libLinear initializes the model, each author is combined with each feature and, then, is assigned an element in the array.  When $ (\text{\# authors} * \text{\# features}) > 2^{31}$, libLinear cannot process that method-feature combination.
		\paragraph*{} To fix this situation, the array of integers in libLinear could be replaced with a vector of integers or list of integers.  Locating a value within the list or vector would be more complex than using an array, so an additional author-feature tracking mechanism might be needed.  Another option would be to change the one dimensional array of integers to a two dimensional array of integers and divide the expected index into the array by $2^{31}$ to determine what row of the two dimensional array should be accessed.
		\paragraph*{} Simply changing the data structure of libLinear would not suffice for adapting libLinear to extremely large method-feature type combination.  The performance impact of using a more complex data structure would need to be measured.  A more complex data structure could slow libLinear to the point of being unusable, which would undermine the purpose of modifying libLinear in the first place.

	\subsection{Apply Good-Turing and Witten-Bell Smoothing to Naive Bayes}
		\paragraph*{} Laplace Smoothing was used for naive Bayes in this thesis. In the case of Web1T\% of 1 and higher, the actual counts within the chosen Web1T\% corpus were used to smooth unseen words in the authors model.   For Web1T\% of 0, a single value was assigned for smoothing based on the feature type used. (There are a large number of GB3 features, so the assigned Laplace Smoothing value was relatively small.  There are relatively few GM1 features, so the assigned Laplace Smoothing value was relatively large.)In the scoring of accuracy versus size, the Web1T\% of 0 produced higher scores than for Web1T\% of 1 and higher.
	\paragraph*{} Since Web1T\% of 0 scored better relative to its more storage intensive counterparts, further exploration is warranted.  Instead of the very basic Laplace Smoothing, Good-Turing or Witten-Bell could be used over all the feature types to see if performance is significantly improved.  Since Good-Turing and Witten-Bell would likely have little impact on the storage requirements for any given feature type, a higher accuracy would result in a higher score ($score=\frac{accuracy}{storage \medspace requirement}$) for that feature type.

	\subsection{Increase Size of the Twitter Short Message Corpus}
		\paragraph*{} The large difference in accuracy and f-score between the Enron E-mail Corpus and Twitter Short Message Corpus may be a function of how few tokens are present in the Twitter Corpus compared to the Enron E-mail Corpus.  If the most prolific Twitter authors could be recorded for several months, a large enough body of tokens could be created to put the token count of some Twitter authors on par with the average Enron e-mail author's token count.  Testing a Twitter corpus with a larger amount of text could clarify whether Twitter is inherently different from e-mail or is simply less predictable when there is a smaller sample to analyze.
		\paragraph*{} A large Twitter corpus would need to find a few hundred Twitter authors who regularly create original content that is publicly accessible.  The Twitter Garden Hose would need to gather Tweets for a few weeks to identify these prolific Twitter authors.  After the initial gathering, the identified prolific Twitter authors would be collected exclusively while screening out re-tweets.  A good faith effort to identify whether any of the prolific twitter authors represented a corporation or public figure known to use a group of writers for their tweets would be needed.  Twitter author attribution only works if there is truly one person creating the content for each Twitter account tracked.
		\paragraph*{} Once there are a few Twitter authors that have approximately 15MB of Twitter text and numerous Twitter authors with at least 500K of Twitter text, the Twitter corpus would be comparable to the Enron E-mail Corpus for size of text and relative author prolificity.  At this point, this new Twitter corpus could be tested again, using all the method-feature combinations of this thesis, to determine if the new Twitter accuracy and f-score improve to more closely resemble the Enron corpus.  If the Twitter results begin to more closely resemble Enron results, then there may not be a significant language ``signal" difference between short message posts and e-mail messages.

\begin{singlespace}
\section{Conduct Further Analysis of Statistics from This Thesis}
\end{singlespace}
	\begin{singlespace}
	\subsection{Investigate Reasons for Higher Accuracy for Naive Bayes Performance in Twitter}
	\end{singlespace}
	\paragraph*{} The relative difference between SVM accuracy and naive Bayes accuracy is much smaller for Twitter than for Enron.  Is this caused by the overall lower accuracies for both SVM and naive Bayes in Twitter?  Is the short length of tweets better suited for naive Bayes?  Answering these questions could provide insight into the language ``signal" of Twitter and the effect of short documents on SVM and naive Bayes.

	\subsection{Study Public Tweets of Former Enron Employees}
	\paragraph*{} If former Enron employees maintain public Twitter posts, their compiled Tweets could be tested against author detection models created from their Enron e-mails.  This could be a straightforward check of language ``signal" similarity between short messages and e-mail.
	\paragraph*{} If any corpus of short message and emails created by the same group of users could be located, conducting author detection by training on one corpus and testing against the other corpus would provide valuable insight into the uniqueness of the email and short message mediums.  Even comparing traditional text such as journals, magazines, book, and newspapers against public Twitter posts could yield valuable insight into author detection across mediums.

	\begin{singlespace}
	\subsection{Statistical Study of Small-To-Large Versus Small-And-Large Groupings Results}
	\end{singlespace}
	\paragraph*{} A detailed study of the impact of author prolificity on author detection accuracy and f-score could reveal important prolificity breakpoints between authors.  By breakpoints, we mean when is an author so prolific that he cannot be placed in a group of less prolific authors without decreasing author detection accuracy for that group. By the same token, when an author so unprolific that they cannot be placed in a group of more prolific authors without decreasing author detection accuracy for that group. Studying breakpoints in the variation between authors in prolificity and its impact on author detection accuracy and f-score would benefit author model construction.

		\subsection{Deliver Author Detection Tools to Mobile Device}
			\paragraph*{} Having a working author detection tool is one step toward deployed implementation. However, that tool still must be installed on mobile devices.  There are different installation methods for varying purposes.  Two major categories of installation can be termed ``deliberate" and ``covert."
			\paragraph*{}An example of a deliberate installation would be a child-predator detector installed on a teen's mobile phone. To support a parent's desire to know if a child-predator is communicating with their teen, simply packaging author detection tools for the Android marketplace is only one step.  Text, authored by local child predators, would need to be gathered and trained into models.  This would be a non-trivial collection and organization effort. Finding an effective strategy for this collection and organization would be a valuable avenue of study.
			\paragraph*{} An example of a covert installation would be saturating mobile devices in a combat operation's area with an author detection tool containing models of high-value enemies. This covert delivery to an unknowing device and user poses many more difficulties than the deliberate installation. Several questions must be answered for a covert delivery:  Is the local cell tower controlled by an independent entity?  Are there popular applications used by the target demographic?  Can the author detection tools be joined with that popular application?  Is it easy to detect the author detection tools once installed?
			\paragraph*{} Each of these delivery categories is worthy of their own study to determine feasibility and to develop efficient and reliable installation methods.  Each of these delivery categories would also need extensive legal and administrative review to ensure compliance with federal, state, and local laws as well as intelligence collection constraints.

\section{Concluding Remarks}
\paragraph*{} With billions of mobile phones across the world, leveraging the power of those billions of processors to identify persons of interest could be of enormous use to governments, organizations, and families.  This thesis has shown that author detection method-feature combinations exist which can be executed in the constrained environment of a mobile device.  With additional testing and engineering, the model of centralized analysis of data collected from distributed mobile devices could be changed dramatically to include distributed collection, distributed processing, and distributed notification.  This distributed model offers great promise for detecting persons of interest via mobile devices.

