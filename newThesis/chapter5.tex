\chapter{Conclusions and Future Work}

\section{Determine Accuracy and F-Score for Other Web1T Vocabulary Variations}
\paragraph{} This thesis only used model 0.  Models such as 8 get rid of punctuation.  Other models get rid of sentence boundaries, capitalization, etc.  Reducing these allowed types reduces the total number of features, which could change accuracy and f-score.

\section{Apply Good Turing or Witten-Bell Smoothing to Naive Bayes}
\paragraph{} Laplace smoothing using a +1 smoothing value for the bootstrap gave poor results.  The Web1T smoothing was to resource intensive.  Good Turing or Witten-Bell might improve Naive Bayes performance without the dramatic overhead of Web1T.

\section{Increase the Twitter Short Message Size}
	\paragraph{} The large difference in accuracy and F-Score between the ENRON Email Corpus and Twitter Short Message Corpus may be a function of how few tokens are present in the Twitter Corpus compared to the ENRON Email Corpus.  If the most prolific Tweeters could be recorded for several months, a large enough body of tokens could be created to put some Tweeter's token count on par with the average ENRON email author's token count.  That could clarify whether Twitter is inherently different from email or is simply less predictable when there is a smaller sample to analyze.

\section{Rewrite LibLinear Data Structures}

\section{Placement on the Mobile Device}

\section{Study of how big a text/email social network usually is to appropriately size the to of the classifier group size}

\section{Study of Disk Storage to RAM Usage for Mobile Phones to support sizing for Dalvik VM or other OS RAM limitations}

\section{Statistical Study of Small-To-Large Versus Small-And-Large Groupings Results}
Could take the most prolific speaker as measured in tokens and compare with most prolific speaker in terms of documents.  Move each of those baseline speaker through different size groupings of other speakers.

\section{Conduct LibLinear and Naive Bayes Tests Again with a Large "Noise" Group}
For Enron, could have 5 identfied authors and all the rest of the authors with a single author ID acting as the noise group.  Cycle through all authors using the small-to-large and small-and-large grouping strategies.  Repeat these experiments for group sizes of 10, 25, 50, and 75.

\section{Actually Test the Top Scoring Method-Feature Combinations on Android Phones}
\paragraph{} 


