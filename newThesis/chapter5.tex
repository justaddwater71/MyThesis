\chapter{Conclusions and Future Work}

\section{Determine Accuracy and F-Score for Other Web1T Vocabulary Variations}
\paragraph{} This thesis only used model 0.  Models such as 8 get rid of punctuation.  Other models get rid of sentence boundaries, capitalization, etc.  Reducing these allowed types reduces the total number of features, which could change accuracy and f-score.

\section{Apply Good Turing or Witten-Bell Smoothing to Naive Bayes}
\paragraph{} Laplace smoothing using a +1 smoothing value for the bootstrap gave poor results.  The Web1T smoothing was to resource intensive.  Good Turing or Witten-Bell might improve Naive Bayes performance without the dramatic overhead of Web1T.

\section{Increase the Twitter Short Message Size}
	\paragraph{} The large difference in accuracy and F-Score between the ENRON Email Corpus and Twitter Short Message Corpus may be a function of how few tokens are present in the Twitter Corpus compared to the ENRON Email Corpus.  If the most prolific Tweeters could be recorded for several months, a large enough body of tokens could be created to put some Tweeter's token count on par with the average ENRON email author's token count.  That could clarify whether Twitter is inherently different from email or is simply less predictable when there is a smaller sample to analyze.

\section{Placement on the Mobile Device}
\paragraph{} 


