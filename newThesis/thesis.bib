
@article{cortes_rational_2004,
	title = {Rational Kernels: Theory and Algorithms},
	volume = {5},
	shorttitle = {Rational Kernels},
	url = {http://portal.acm.org/citation.cfm?id=1005332.1016793&coll=ACM&dl=ACM&CFID=96681423&CFTOKEN=43711541},
	abstract = {Many classification algorithms were originally designed for fixed-size vectors. Recent applications in text and speech processing and computational biology require however the analysis of variable-length sequences and more generally weighted automata. An approach widely used in statistical learning techniques such as Support Vector Machines {(SVMs)} is that of kernel methods, due to their computational efficiency in high-dimensional feature spaces. We introduce a general family of kernels based on weighted transducers or rational relations, rational kernels , that extend kernel methods to the analysis of variable-length sequences or more generally weighted automata. We show that rational kernels can be computed efficiently using a general algorithm of composition of weighted transducers and a general single-source shortest-distance algorithm. Not all rational kernels are positive definite and symmetric {(PDS),} or equivalently verify the Mercer condition, a condition that guarantees the convergence of training for discriminant classification algorithms such as {SVMs.} We present several theoretical results related to {PDS} rational kernels. We show that under some general conditions these kernels are closed under sum, product, or Kleene-closure and give a general method for constructing a {PDS} rational kernel from an arbitrary transducer defined on some non-idempotent semirings. We give the proof of several characterization results that can be used to guide the design of {PDS} rational kernels. We also show that some commonly used string kernels or similarity measures such as the edit-distance, the convolution kernels of Haussler, and some string kernels used in the context of computational biology are specific instances of rational kernels. Our results include the proof that the edit-distance over a non-trivial alphabet is not negative definite, which, to the best of our knowledge, was never stated or proved before. Rational kernels can be combined with {SVMs} to form efficient and powerful techniques for a variety of classification tasks in text and speech processing, or computational biology. We describe examples of general families of {PDS} rational kernels that are useful in many of these applications and report the result of our experiments illustrating the use of rational kernels in several difficult large-vocabulary spoken-dialog classification tasks based on deployed spoken-dialog systems. Our results show that rational kernels are easy to design and implement and lead to substantial improvements of the classification accuracy.},
	journal = {J. Mach. Learn. Res.},
	author = {Corinna Cortes and Patrick Haffner and Mehryar Mohri},
	year = {2004},
	pages = {1035--1062}
},

@misc{_twitter_????,
	title = {Twitter Blog: Measuring Tweets},
	url = {http://blog.twitter.com/2010/02/measuring-tweets.html},
	howpublished = {http://blog.twitter.com/2010/02/measuring-tweets.html}
},

@inproceedings{watanabe_succinct_2009,
	address = {Stroudsburg, {PA,} {USA}},
	series = {{ACLShort} '09},
	title = {A succinct N-gram language model},
	location = {Suntec, Singapore},
	url = {http://portal.acm.org/citation.cfm?id=1667583.1667689},
	abstract = {Efficient processing of tera-scale text data is an important research topic. This paper proposes lossless compression of N-gram language models based on {LOUDS,} a succinct data structure. {LOUDS} succinctly represents a trie with M nodes as a {2M} + 1 bit string. We compress it further for the N-gram language model structure. We also use 'variable length coding' and 'block-wise compression' to compress values associated with nodes. Experimental results for three large-scale N-gram compression tasks achieved a significant compression rate without any loss.},
	booktitle = {Proceedings of the {ACL-IJCNLP} 2009 Conference Short Papers},
	publisher = {Association for Computational Linguistics},
	author = {Taro Watanabe and Hajime Tsukada and Hideki Isozaki},
	year = {2009},
	note = {{ACM} {ID:} 1667689},
	keywords = {Algorithms, data compaction and compression, design, language models, languages, modeling methodologies, performance},
	pages = {341{\textendash}344}
},

@misc{_naive_????,
	title = {Naive Bayes text classification},
	url = {http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html},
	howpublished = {{http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html}}
},

@book{murphy_android_2010,
	title = {Android Beyond Java},
	isbn = {9780981678047},
	publisher = {{CommonsWare,} {LLC}},
	author = {Mark Lawrence Murphy},
	month = sep,
	year = {2010}
},

@article{broder_network_2004,
	title = {Network applications of bloom filters: A survey},
	volume = {1},
	issn = {1542-7951},
	shorttitle = {Network applications of bloom filters},
	number = {4},
	journal = {Internet Mathematics},
	author = {A. Broder and M. Mitzenmacher},
	year = {2004},
	pages = {485{\textendash}509}
},

@inproceedings{sculley_relaxed_2007,
	address = {Amsterdam, The Netherlands},
	title = {Relaxed online {SVMs} for spam filtering},
	isbn = {978-1-59593-597-7},
	url = {http://portal.acm.org/citation.cfm?id=1277741.1277813&coll=ACM&dl=ACM&CFID=96681423&CFTOKEN=43711541},
	doi = {10.1145/1277741.1277813},
	abstract = {Spam is a key problem in electronic communication, including large-scale email systems and the growing number of blogs. Content-based filtering is one reliable method of combating this threat in its various forms, but some academic researchers and industrial practitioners disagree on how best to filter spam. The former have advocated the use of Support Vector Machines {(SVMs)} for content-based filtering, as this machine learning methodology gives state-of-the-art performance for text classification. However, similar performance gains have yet to be demonstrated for online spam filtering. Additionally, practitioners cite the high cost of {SVMs} as reason to prefer faster (if less statistically robust) Bayesian methods. In this paper, we offer a resolution to this controversy. First, we show that online {SVMs} indeed give state-of-the-art classification performance on online spam filtering on large benchmark data sets. Second, we show that nearly equivalent performance may be achieved by a Relaxed Online {SVM} {(ROSVM)} at greatly reduced computational cost. Our results are experimentally verified on email spam, blog spam, and splog detection tasks.},
	booktitle = {Proceedings of the 30th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {D. Sculley and Gabriel M. Wachman},
	year = {2007},
	keywords = {blogs, spam filtering, splogs, support vector machines},
	pages = {415--422}
},

@misc{brants_web_2006,
	title = {{{Web} {1T} 5-gram Version 1}},
	publisher = {Linguistic Data Consortium, Philadelphia},
	author = {Thorsten Brants and Alex Franz},
	year = {2006},
	keywords = {ir, tool}
},

@inproceedings{islam_managing_2009,
	title = {Managing the Google Web {1T} 5-gram data set},
	doi = {10.1109/NLPKE.2009.5313839},
	abstract = {This paper describes how the Google Web {1T} 5-gram data set, contributed by Google Inc., can be stored so that it can be used efficiently with respect to time. We present an efficient way of accessing all the 5-grams for a specific word of interest from the stored files. We measure the maximum access and processing efficiency achievable for any word of interest. We also compare results (access time and memory requirements) on the task of accessing all the 5-grams for a list of words, on both the processed and the original organization of the data set.},
	booktitle = {Natural Language Processing and Knowledge Engineering, 2009. {NLP-KE} 2009. International Conference on},
	author = {A. Islam and D. Inkpen},
	year = {2009},
	keywords = {data handling, Google Incorporated, Google Web {1T} 5-gram data set, Internet, natural language processing},
	pages = {1--5}
},

@book{amati_advances_2007,
	address = {Berlin, Heidelberg},
	title = {Advances in Information Retrieval},
	volume = {4425},
	isbn = {978-3-540-71494-1},
	url = {http://www.springerlink.com.libproxy.nps.edu/content/q1513k3778644204/},
	publisher = {Springer Berlin Heidelberg},
	editor = {Giambattista Amati and Claudio Carpineto and Giovanni Romano},
	year = {2007}
},

@article{klimt_introducing_????,
	title = {Introducing the Enron Corpus},
	author = {B. Klimt and Y. Yang}
},

@book{murphy_busy_2010,
	title = {The Busy Coder's Guide to Android Development},
	isbn = {9780981678009},
	publisher = {{CommonsWare}},
	author = {Mark L. Murphy},
	month = oct,
	year = {2010}
},

@inproceedings{chellapilla_gigahash:_2007,
	address = {Banff, Alberta, Canada},
	title = {{GigaHash:} scalable minimal perfect hashing for billions of urls},
	isbn = {978-1-59593-654-7},
	shorttitle = {{GigaHash}},
	url = {http://portal.acm.org/citation.cfm?id=1242572.1242747&coll=ACM&dl=ACM&CFID=99170154&CFTOKEN=68143502},
	doi = {10.1145/1242572.1242747},
	abstract = {A minimal perfect function maps a static set of n keys on to the range of integers {0,1,2,...,n - 1}. We present a scalable high performance algorithm based on random graphs for constructing minimal perfect hash functions {(MPHFs).} For a set of n keys, our algorithm outputs a description of h in expected time O(n). The evaluation of h(x) requires three memory accesses for any key x and the description of h takes up 0.89n bytes (7.13n bits). This is the best (most space efficient) known result to date. Using a simple heuristic and Huffman coding, the space requirement is further reduced to 0.79n bytes (6.86n bits). We present a high performance architecture that is easy to parallelize and scales well to very large data sets encountered in internet search applications. Experimental results on a one billion {URL} dataset obtained from Live Search crawl data, show that the proposed algorithm (a)finds an {MPHF} for one billion {URLs} in less than 4 minutes, and (b) requires only 6.86 bits/key for the description of h.},
	booktitle = {Proceedings of the 16th international conference on World Wide Web},
	publisher = {{ACM}},
	author = {Kumar Chellapilla and Anton Mityagin and Denis Charles},
	year = {2007},
	keywords = {minimal perfect hashing, perfect hash function, space efficient hash table, web search engine},
	pages = {1165--1166}
},

@inproceedings{bikel_if_2007,
	address = {Washington, {DC,} {USA}},
	title = {If We Want Your Opinion},
	isbn = {0-7695-2997-6},
	url = {http://portal.acm.org/citation.cfm?id=1304608.1306375},
	doi = {10.1109/ICSC.2007.40},
	abstract = {Sentiment has traditionally been considered a "deep" attribute of writing, often requiring the interpretation of figurative language to uncover the writer's intention. The natural language processing community has become increasingly interested in detecting, through automatic means, the expression of opinions and measuring the intensity of emotions held by the writer. Despite the depth and abstraction often associated with expressions of sentiment, we apply strictly lexical analysis to the opinions expressed about books and find that machine learning techniques are capable of resolving even fine-grained distinctions between opinions. Using an averaged perceptron classifier trained using a word subsequence kernel, we achieve an accuracy of 89\% when distinguishing between 1- and 5-star reviews. Further, this same model yields significant separation when scoring intermediate reviews--making distinctions even human annotators find difficult. We detail the collection of data for supervised training and present the results of our sentiment classifier along with some discussion about why we believe this approach to be effective.},
	booktitle = {Proceedings of the International Conference on Semantic Computing},
	publisher = {{IEEE} Computer Society},
	author = {Daniel M Bikel and Jeffrey Sorensen},
	year = {2007},
	note = {{ACM} {ID:} 1306375},
	keywords = {classifier design and evaluation, design, experimentation, language parsing and understanding, measurement, performance},
	pages = {493{\textendash}500}
},

@inproceedings{estival_tat:_2007,
	title = {{TAT:} an author profiling tool with application to Arabic emails},
	shorttitle = {{TAT}},
	booktitle = {Proceedings of the Australasian Language Technology Workshop},
	author = {D. Estival and T. Gaustad and S. B Pham and W. Radford and B. Hutchinson},
	year = {2007},
	pages = {21{\textendash}30}
},

@inproceedings{yuret_smoothing_2008,
	address = {Stroudsburg, {PA,} {USA}},
	series = {{HLT-Short} '08},
	title = {Smoothing a tera-word language model},
	location = {Columbus, Ohio},
	url = {http://portal.acm.org/citation.cfm?id=1557690.1557727},
	abstract = {Frequency counts from very large corpora, such as the Web {1T} dataset, have recently become available for language modeling. Omission of low frequency n-gram counts is a practical necessity for datasets of this size. Naive implementations of standard smoothing methods do not realize the full potential of such large datasets with missing counts. In this paper I present a new smoothing algorithm that combines the Dirichlet prior form of {(Mackay} and Peto, 1995) with the modified back-off estimates of {(Kneser} and Ney, 1995) that leads to a 31\% perplexity reduction on the Brown corpus compared to a baseline implementation of {Kneser-Ney} discounting.},
	booktitle = {Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers},
	publisher = {Association for Computational Linguistics},
	author = {Deniz Yuret},
	year = {2008},
	note = {{ACM} {ID:} 1557727},
	keywords = {Algorithms, design, modeling methodologies, natural language processing, theory},
	pages = {141{\textendash}144}
},

@book{jurafsky_speech_2009,
	title = {Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition},
	isbn = {9780131873216},
	shorttitle = {Speech and language processing},
	publisher = {Prentice Hall},
	author = {Daniel Jurafsky and James H. Martin},
	year = {2009}
},

@inproceedings{li_support_2007,
	address = {Corvalis, Oregon},
	title = {Support cluster machine},
	isbn = {978-1-59593-793-3},
	url = {http://portal.acm.org/citation.cfm?id=1273496.1273560&coll=GUIDE&dl=GUIDE&CFID=98984342&CFTOKEN=71344517},
	doi = {10.1145/1273496.1273560},
	abstract = {For large-scale classification problems, the training samples can be clustered beforehand as a downsampling pre-process, and then only the obtained clusters are used for training. Motivated by such assumption, we proposed a classification algorithm, Support Cluster Machine {(SCM),} within the learning framework introduced by Vapnik. For the {SCM,} a compatible kernel is adopted such that a similarity measure can be handled not only between clusters in the training phase but also between a cluster and a vector in the testing phase. We also proved that the {SCM} is a general extension of the {SVM} with the {RBF} kernel. The experimental results confirm that the {SCM} is very effective for largescale classification problems due to significantly reduced computational costs for both training and testing and comparable classification accuracies. As a by-product, it provides a promising approach to dealing with privacy-preserving data mining problems.},
	booktitle = {Proceedings of the 24th international conference on Machine learning},
	publisher = {{ACM}},
	author = {Bin Li and Mingmin Chi and Jianping Fan and Xiangyang Xue},
	year = {2007},
	pages = {505--512}
},

@article{smith_mobile_2010,
	title = {Mobile access 2010},
	volume = {8},
	journal = {Pew Internet and American Life Project. {http://pewInternet.} {org/Reports/2010/Mobile-Access-2010.} aspx. Accessed August},
	author = {A. Smith},
	year = {2010},
	pages = {2010}
},

@misc{_cmph_????,
	title = {{CMPH} - C Minimal Perfect Hashing Library},
	url = {http://cmph.sourceforge.net/},
	howpublished = {http://cmph.sourceforge.net/}
},

@article{hsu_practical_????,
	title = {A Practical Guide to Support Vector Classification},
	author = {C. W Hsu and C. C Chang and C. J Lin}
},

@incollection{schmidt_gperf:_2000,
	title = {{GPERF:} a perfect hash function generator},
	isbn = {0-521-78618-5},
	shorttitle = {{GPERF}},
	url = {http://portal.acm.org/citation.cfm?id=331120.331202&coll=GUIDE&dl=GUIDE&CFID=98945654&CFTOKEN=58295763},
	booktitle = {More C++ gems},
	publisher = {Cambridge University Press},
	author = {Douglas C. Schmidt},
	year = {2000},
	pages = {461--491}
},

@inproceedings{layton_authorship_2010,
	address = {Los Alamitos, {CA,} {USA}},
	title = {Authorship Attribution for Twitter in 140 Characters or Less},
	volume = {0},
	isbn = {978-0-7695-4186-0},
	doi = {http://doi.ieeecomputersociety.org/10.1109/CTC.2010.17},
	abstract = {Authorship attribution is a growing field, moving from beginnings in linguistics to recent advances in text mining. Through this change came an increase in the capability of authorship attribution methods both in their accuracy and the ability to consider more difficult problems. Research into authorship attribution in the 19th century considered it difficult to determine the authorship of a document of fewer than 1000 words. By the 1990s this values had decreased to less than 500 words and in the early 21st century it was considered possible to determine the authorship of a document in 250 words. The need for this ever decreasing limit is exemplified by the trend towards many shorter communications rather than fewer longer communications, such as the move from traditional multi-page handwritten letters to shorter, more focused emails. This trend has also been shown in online crime, where many attacks such as phishing or bullying are performed using very concise language. Cybercrime messages have long been hosted on Internet Relay Chats {(IRCs)} which have allowed members to hide behind screen names and connect anonymously. More recently, Twitter and other short message based web services have been used as a hosting ground for online crimes. This paper presents some evaluations of current techniques and identifies some new preprocessing methods that can be used to enable authorship to be determined at rates significantly better than chance for documents of 140 characters or less, a format popularised by the micro-blogging website Twitter1. We show that the {SCAP} methodology performs extremely well on twitter messages and even with restrictions on the types of information allowed, such as the recipient of directed messages, still perform significantly higher than chance. Further to this, we show that 120 tweets per user is an important threshold, at which point adding more tweets per user gives a small but non-significant increase in accuracy.},
	booktitle = {Cybercrime and Trustworthy Computing, Workshop},
	publisher = {{IEEE} Computer Society},
	author = {Robert Layton and Paul Watters and Richard Dazeley},
	year = {2010},
	pages = {1--8},
	annote = {Complete {PDF} document was either not available or accessible. Please make sure you're logged in to the digital library to retrieve the complete {PDF} document.}
},

@inproceedings{daoud_perfect_2007,
	address = {Lisbon, Portugal},
	title = {Perfect hash functions for large dictionaries},
	isbn = {978-1-59593-831-2},
	url = {http://portal.acm.org/citation.cfm?doid=1317353.1317368},
	doi = {10.1145/1317353.1317368},
	abstract = {We describe a new practical algorithm for finding perfect hash functions with no specification space at all, suitable for key sets ranging in size from small to very large. The method is able to find perfect hash functions for various sizes of key sets in linear time. The perfect hash functions produced are optimal in terms of time (perfect) and require at most computation of h1(k) and h2(k); two simple auxiliary pseudorandom functions.},
	booktitle = {Proceedings of the {ACM} first workshop on {CyberInfrastructure:} information management in {eScience}},
	publisher = {{ACM}},
	author = {Amjad M. Daoud},
	year = {2007},
	keywords = {acyclic, indexing, mos, perfect hashing, random graphs},
	pages = {67--72}
},

@misc{_symbian_????,
	title = {Symbian {SDKs}},
	url = {http://www.forum.nokia.com/info/sw.nokia.com/id/ec866fab-4b76-49f6-b5a5-af0631419e9c/S60_All_in_One_SDKs.html},
	howpublished = {{http://www.forum.nokia.com/info/sw.nokia.com/id/ec866fab-4b76-49f6-b5a5-af0631419e9c/S60\_All\_in\_One\_SDKs.html}}
},

@article{keselj_n-gram-based_2003,
	title = {N-gram-based author profiles for authorship attribution},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.9.7388},
	author = {Vlado Keselj and Fuchun Peng and Nick Cercone and Calvin Thomas},
	year = {2003}
},

@inproceedings{tang_email_2005,
	address = {Chicago, Illinois, {USA}},
	title = {Email data cleaning},
	isbn = {{1-59593-135-X}},
	url = {http://portal.acm.org/citation.cfm?id=1081926},
	doi = {10.1145/1081870.1081926},
	abstract = {Addressed in this paper is the issue of 'email data cleaning' for text mining. Many text mining applications need take emails as input. Email data is usually noisy and thus it is necessary to clean it before mining. Several products offer email cleaning features, however, the types of noises that can be eliminated are restricted. Despite the importance of the problem, email cleaning has received little attention in the research community. A thorough and systematic investigation on the issue is thus needed. In this paper, email cleaning is formalized as a problem of non-text filtering and text normalization. In this way, email cleaning becomes independent from any specific text mining processing. A cascaded approach is proposed, which cleans up an email in four passes including non-text filtering, paragraph normalization, sentence normalization, and word normalization. As far as we know, non-text filtering and paragraph normalization have not been investigated previously. Methods for performing the tasks on the basis of Support Vector Machines {(SVM)} have also been proposed in this paper. Features in the models have been defined. Experimental results indicate that the proposed {SVM} based methods can significantly outperform the baseline methods for email cleaning. The proposed method has been applied to term extraction, a typical text mining processing. Experimental results show that the accuracy of term extraction can be significantly improved by using the data cleaning method.},
	booktitle = {Proceedings of the eleventh {ACM} {SIGKDD} international conference on Knowledge discovery in data mining},
	publisher = {{ACM}},
	author = {Jie Tang and Hang Li and Yunbo Cao and Zhaohui Tang},
	year = {2005},
	keywords = {data cleaning, email processing, statistical learning, text mining},
	pages = {489--498}
},

@phdthesis{boutwell_sarah_author_2011,
	address = {Monterey, {CA}},
	title = {Author Attribution Using Twitter and Mobile Phone Signal Characteristics},
	school = {Naval Postgraduate School},
	author = {Boutwell, Sarah},
	month = mar,
	year = {2011}
},

@article{fan_liblinear:_2008,
	title = {{LIBLINEAR:} A Library for Large Linear Classification},
	volume = {9},
	shorttitle = {{LIBLINEAR}},
	url = {http://portal.acm.org/citation.cfm?id=1390681.1442794&coll=ACM&dl=ACM&CFID=96681423&CFTOKEN=43711541},
	abstract = {{LIBLINEAR} is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that {LIBLINEAR} is very efficient on large sparse data sets.},
	journal = {J. Mach. Learn. Res.},
	author = {{Rong-En} Fan and {Kai-Wei} Chang and {Cho-Jui} Hsieh and {Xiang-Rui} Wang and {Chih-Jen} Lin},
	year = {2008},
	pages = {1871--1874}
},

@article{mosteller_inference_1963,
	title = {Inference in an Authorship Problem},
	volume = {58},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2283270},
	doi = {10.2307/2283270},
	abstract = {This study has four purposes: to provide a comparison of discrimination methods; to explore the problems presented by techniques based strongly on Bayes' theorem when they are used in a data analysis of large scale; to solve the authorship question of The Federalist papers; and to propose routine methods for solving other authorship problems. Word counts are the variables used for discrimination. Since the topic written about heavily influences the rate with which a word is used, care in selection of words is necessary. The filler words of the language such as an, of, and upon, and, more generally, articles, prepositions, and conjunctions provide fairly stable rates, whereas more meaningful words like war, executive, and legislature do not. After an investigation of the distribution of these counts, the authors execute an analysis employing the usual discriminant function and an analysis based on Bayesian methods. The conclusions about the authorship problem are that Madison rather than Hamilton wrote all 12 of the disputed papers. The findings about methods are presented in the closing section on conclusions. This report, summarizing and abbreviating a forthcoming monograph [8], gives some of the results but very little of their empirical and theoretical foundation. It treats two of the four main studies presented in the monograph, and none of the side studies.},
	number = {302},
	journal = {Journal of the American Statistical Association},
	author = {Frederick Mosteller and David L. Wallace},
	month = jun,
	year = {1963},
	note = {{ArticleType:} research-article / Full publication date: Jun., 1963 / Copyright {\textcopyright} 1963 American Statistical Association},
	pages = {275--309}
},

@misc{_enron_????,
	title = {Enron Email Dataset},
	url = {http://www-2.cs.cmu.edu/%7Eenron/},
	howpublished = {{http://www-2.cs.cmu.edu/\%7Eenron/}}
},

@article{bloom_space/time_1970,
	title = {Space/time trade-offs in hash coding with allowable errors},
	volume = {13},
	issn = {0001-0782},
	number = {7},
	journal = {Communications of the {ACM}},
	author = {B. H Bloom},
	year = {1970},
	pages = {422{\textendash}426}
},

@misc{_creating_????,
	title = {Creating an {iPhone} Application},
	url = {http://developer.apple.com/library/ios/ #referencelibrary/ GettingStarted/ Creating_an_iPhone_App/ index.html},
	journal = {Creating an {iPhone} Application},
	howpublished = {http://developer.apple.com/library/ios/ \#referencelibrary/ {GettingStarted/} {Creating\_an\_iPhone\_App/} index.html}
},

@article{pagh_hash_1999,
	title = {Hash and displace: Efficient evaluation of minimal perfect hash functions},
	shorttitle = {Hash and displace},
	journal = {Algorithms and Data Structures},
	author = {R. Pagh},
	year = {1999},
	pages = {767{\textendash}767}
},

@inproceedings{guthrie_minimal_2010,
	title = {Minimal Perfect Hash Rank: Compact Storage of Large N-gram Language Models},
	shorttitle = {Minimal Perfect Hash Rank},
	booktitle = {Web N-gram Workshop},
	author = {D. Guthrie and M. Hepple},
	year = {2010},
	pages = {29}
},

@inproceedings{yang_improved_2006,
	title = {An Improved Cascade {SVM} Training Algorithm with Crossed Feedbacks},
	isbn = {0-7695-2581-4},
	url = {http://portal.acm.org/citation.cfm?id=1136466},
	abstract = {Support Vector Machine {(SVM)} has become a popular classification tool but one of its disadvantages is large memory requirement and computation time when dealing with large datasets. Parallel methods have been proposed to speed up the process of training {SVM.} An improved cascade {SVM} training algorithm is proposed, in which multiple {SVM} classifiers are applied. The support vectors are obtained by feeding back in a crossed way, alternating to avoid the problem that the learning results are subject to the distribution state of the data samples in different subsets. The experiment results on {UCI} dataset show that this parallel {SVM} training algorithm is efficient and has more satisfying accuracy compared with standard cascade {SVM} algorithm in classification precision.},
	booktitle = {Proceedings of the First International {Multi-Symposiums} on Computer and Computational Sciences - Volume 2  {(IMSCCS'06)} - Volume 02},
	publisher = {{IEEE} Computer Society},
	author = {Jing Yang},
	year = {2006},
	pages = {735--738},
	annote = {{{\textless}p{\textgreater}Paper} is at {IEEE} vice {ACM.}~ {VPN} access did not get me access to the library will try through actual {BOSUN} next.{\textless}/p{\textgreater}}
},

@misc{_streaming_????,
	title = {Streaming {API} Documentation {\textbar} dev.twitter.com},
	url = {http://dev.twitter.com/pages/streaming_api},
	journal = {Streaming {API} Documentation},
	howpublished = {http://dev.twitter.com/pages/streaming\_api}
},

@book{manning_introduction_2008,
	title = {Introduction to information retrieval},
	isbn = {9780521865715},
	publisher = {Cambridge University Press},
	author = {Christopher D. Manning and Prabhakar Raghavan and Hinrich Sch\"{u}tze},
	year = {2008}
},

@misc{_gartner_????,
	title = {Gartner Says Worldwide Mobile Phone Sales Grew 17 Per Cent in First Quarter 2010},
	url = {http://www.gartner.com/it/page.jsp?id=1372013},
	journal = {Gartner Says Worldwide Mobile Phone Sales Grew 17 Per Cent in First Quarter 2010},
	howpublished = {http://www.gartner.com/it/page.jsp?id=1372013}
},

@article{jenkins_hash_1997,
	title = {Hash functions},
	volume = {22},
	issn = {{1044789X}},
	abstract = {Jenkins discusses how hash functions can fail and uses this principle to design a new hash function.},
	number = {9},
	journal = {Dr. Dobb's Journal},
	author = {Bob Jenkins},
	month = sep,
	year = {1997},
	keywords = {Algorithms},
	pages = {107}
},

@misc{_method_2010,
	title = {Method and system for detection of authors},
	url = {http://www.google.com/patents?hl=en&lr=&vid=USPAT7752208&id=quXRAAAAEBAJ&oi=fnd&dq=%22author+detection%22&printsec=abstract},
	month = jul,
	year = {2010},
	note = {{undefinedFiling} Date: Apr 11, 2007}
},

@inproceedings{gauravaram_update_2007,
	address = {Chennai, India},
	title = {An update on the side channel cryptanalysis of {MACs} based on cryptographic hash functions},
	isbn = {3-540-77025-9, 978-3-540-77025-1},
	url = {http://portal.acm.org/citation.cfm?id=1777898.1777939&coll=ACM&dl=ACM&CFID=96859924&CFTOKEN=42380848},
	abstract = {Okeya has established that {HMAC/NMAC} implementations based on only {Matyas-Meyer-Oseas} {(MMO)} {PGV} scheme and his two refined {PGV} schemes are secure against side channel {DPA} attacks when the block cipher in these constructions is secure against these attacks. The significant result of Okeya's analysis is that the implementations of {HMAC/NMAC} with the {Davies-Meyer} {(DM)} compression function based hash functions such as {SHA-1} are vulnerable to {DPA} attacks. In this paper, first we show a partial key recovery attack on {NMAC/HMAC} based on Okeya's two refined {PGV} schemes by taking practical constraints into consideration. Next, we propose new hybrid {NMAC/HMAC} schemes for security against side channel attacks assuming that their underlying block cipher is ideal. We show a hybrid {NMAC/HMAC} proposal which can be instantiated with {DM} and a slight variant to it allowing {NMAC/HMAC} to use hash functions such as {SHA-1.} We then show that {M-NMAC,} {MDx-MAC} and a variant of the envelope {MAC} scheme based on {DM} with an ideal block cipher are secure against {DPA} attacks.},
	booktitle = {Proceedings of the cryptology 8th international conference on Progress in cryptology},
	publisher = {{Springer-Verlag}},
	author = {Praveen Gauravaram and Katsuyuki Okeya},
	year = {2007},
	keywords = {dpa, hmac, mdx-mac, m-nmac, side channel attacks},
	pages = {393--403}
},

@article{fisher_use_1936,
	title = {{{The} use of multiple measurements in taxonomic problems}},
	volume = {7},
	number = {2},
	journal = {Annals of Eugenics},
	author = {{RA} Fisher},
	year = {1936},
	keywords = {agent, data-mining},
	pages = {179--188}
},

@inproceedings{koppel_authorship_2004,
	address = {New York, {NY,} {USA}},
	series = {{ICML} '04},
	title = {Authorship verification as a one-class classification problem},
	isbn = {1-58113-838-5},
	location = {Banff, Alberta, Canada},
	doi = {10.1145/1015330.1015448},
	abstract = {In the authorship verification problem, we are given examples of the writing of a single author and are asked to determine if given long texts were or were not written by this author. We present a new learning-based method for adducing the "depth of difference" between two example sets and offer evidence that this method solves the authorship verification problem with very high accuracy. The underlying idea is to test the rate of degradation of the accuracy of learned models as the best features are iteratively dropped from the learning process.},
	booktitle = {Proceedings of the twenty-first international conference on Machine learning},
	publisher = {{ACM}},
	author = {Moshe Koppel and Jonathan Schler},
	year = {2004},
	note = {{ACM} {ID:} 1015448},
	pages = {62{\textendash}}
},

@article{graf_parallel_????,
	title = {Parallel support vector machines: The cascade svm},
	shorttitle = {Parallel support vector machines},
	author = {H. P Graf and E. Cosatto and L. Bottou and V. Vapnik}
},

@misc{_u.s._????,
	title = {{U.S.} Wireless Quick Facts},
	url = {http://www.ctia.org/advocacy/research/index.cfm/aid/10323},
	howpublished = {http://www.ctia.org/advocacy/research/index.cfm/aid/10323}
},

@inproceedings{yu_large_2010,
	title = {Large linear classification when data cannot fit in memory},
	booktitle = {Proceedings of the 16th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	author = {H. F Yu and C. J Hsieh and K. W Chang and C. J Lin},
	year = {2010},
	pages = {833{\textendash}842}
},

@inproceedings{o_seaghdha_semantic_2008,
	address = {Stroudsburg, {PA,} {USA}},
	series = {{COLING} '08},
	title = {Semantic classification with distributional kernels},
	isbn = {978-1-905593-44-6},
	location = {Manchester, United Kingdom},
	url = {http://portal.acm.org/citation.cfm?id=1599081.1599163},
	abstract = {Distributional measures of lexical similarity and kernel methods for classification are well-known tools in Natural Language Processing. We bring these two methods together by introducing distributional kernels that compare co-occurrence probability distributions. We demonstrate the effectiveness of these kernels by presenting state-of-the-art results on datasets for three semantic classification: compound noun interpretation, identification of semantic relations between nominals and semantic classification of verbs. Finally, we consider explanations for the impressive performance of distributional kernels and sketch some promising generalisations.},
	booktitle = {Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1},
	publisher = {Association for Computational Linguistics},
	author = {Diarmuid \'{O} S\'{e}aghdha and Ann Copestake},
	year = {2008},
	note = {{ACM} {ID:} 1599163},
	keywords = {Algorithms, classifier design and evaluation, design, knowledge acquisition, modeling methodologies, natural language processing, performance, semantic networks},
	pages = {649{\textendash}656}
},

@article{germann_tightly_????,
	title = {Tightly Packed Tries: How to Fit Large Models into Memory, and Make them Load Fast, Too},
	shorttitle = {Tightly Packed Tries},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.164.3395},
	author = {Ulrich Germann and Eric Joanis and Samuel Larkin}
},

@article{zoellick_fragile_2008,
	title = {Fragile States: Securing Development},
	volume = {50},
	issn = {0039-6338},
	shorttitle = {Fragile States},
	url = {http://www.informaworld.com/10.1080/00396330802601859},
	doi = {10.1080/00396330802601859},
	number = {6},
	journal = {Survival: Global Politics and Strategy},
	author = {Robert B. Zoellick},
	year = {2008},
	pages = {67}
},

@article{sokolova_beyond_2006,
	title = {Beyond accuracy, f-score and roc: a family of discriminant measures for performance evaluation},
	shorttitle = {Beyond accuracy, f-score and roc},
	journal = {{AI} 2006: Advances in Artificial Intelligence},
	author = {M. Sokolova and N. Japkowicz and S. Szpakowicz},
	year = {2006},
	pages = {1015{\textendash}1021}
},

@article{putze_cache-_2009,
	title = {Cache-, hash-, and space-efficient bloom filters},
	volume = {14},
	url = {http://portal.acm.org/citation.cfm?id=1498698.1594230&coll=ACM&dl=ACM&CFID=99170154&CFTOKEN=68143502},
	doi = {10.1145/1498698.1594230},
	abstract = {A Bloom filter is a very compact data structure that supports approximate membership queries on a set, allowing false positives.},
	journal = {J. Exp. Algorithmics},
	author = {Felix Putze and Peter Sanders and Johannes Singler},
	year = {2009},
	keywords = {approximate dictionary, data compression},
	pages = {4.4--4.18}
},

@article{vapnik_support-vector_1995,
	title = {{Support-Vector} Networks},
	volume = {20},
	issn = {0885-6125},
	url = {http://dx.doi.org/10.1023/A:1022627411411},
	journal = {Machine Learning},
	author = {Vladimir Vapnik and Corinna Cortes},
	year = {1995},
	note = {{10.1023/A:1022627411411}},
	pages = {273--297}
},

@misc{_blackberry_????,
	title = {{BlackBerry} - {BlackBerry} Developer Zone},
	url = {http://us.blackberry.com/developers/},
	journal = {{BlackBerry} Developer Zone},
	howpublished = {http://us.blackberry.com/developers/}
},

@inproceedings{islam_real-word_2009,
	address = {Stroudsburg, {PA,} {USA}},
	series = {{EMNLP} '09},
	title = {Real-word spelling correction using Google Web {IT} 3-grams},
	isbn = {978-1-932432-63-3},
	location = {Singapore},
	url = {http://portal.acm.org/citation.cfm?id=1699648.1699670},
	abstract = {We present a method for detecting and correcting multiple real-word spelling errors using the Google Web {IT} 3-gram data set and a normalized and modified version of the Longest Common Subsequence {(LCS)} string matching algorithm. Our method is focused mainly on how to improve the detection recall (the fraction of errors correctly detected) and the correction recall (the fraction of errors correctly amended), while keeping the respective precisions (the fraction of detections or amendments that are correct) as high as possible. Evaluation results on a standard data set show that our method outperforms two other methods on the same task.},
	booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3},
	publisher = {Association for Computational Linguistics},
	author = {Aminul Islam and Diana Inkpen},
	year = {2009},
	note = {{ACM} {ID:} 1699670},
	keywords = {Algorithms, design, language acquisition, language parsing and understanding, languages, linguistic processing, performance, performance evaluation},
	pages = {1241{\textendash}1249}
},

@article{talbot_randomized_2008,
	title = {Randomized language models via perfect hash functions},
	journal = {Proceedings of {ACL-08:} {HLT}},
	author = {D. Talbot and T. Brants},
	year = {2008},
	pages = {505{\textendash}513}
},

@inproceedings{botelho_external_2007,
	address = {Lisbon, Portugal},
	title = {External perfect hashing for very large key sets},
	isbn = {978-1-59593-803-9},
	url = {http://portal.acm.org/citation.cfm?id=1321440.1321532&coll=ACM&dl=ACM&CFID=99170154&CFTOKEN=68143502},
	doi = {10.1145/1321440.1321532},
	abstract = {We present a simple and efficient external perfect hashing scheme (referred to as {EPH} algorithm) for very large static key sets. We use a number of techniques from the literature to obtain a novel scheme that is theoretically well-understood and at the same time achieves an order-of-magnitude increase in the size of the problem to be solved compared to previous "practical" methods. We demonstrate the scalability of our algorithm by constructing minimum perfect hash functions for a set of 1.024 billion {URLs} from the World Wide Web of average length 64 characters in approximately 62 minutes, using a commodity {PC.} Our scheme produces minimal perfect hash functions using approximately 3.8 bits per key. For perfect hash functions in the range {0,...,2n - 1} the space usage drops to approximately 2.7 bits per key. The main contribution is the first algorithm that has experimentally proven practicality for sets in the order of billions of keys and has time and space usage carefully analyzed without unrealistic assumptions.},
	booktitle = {Proceedings of the sixteenth {ACM} conference on Conference on information and knowledge management},
	publisher = {{ACM}},
	author = {Fabiano C. Botelho and Nivio Ziviani},
	year = {2007},
	keywords = {functions, hash, key sets, large, minimal, perfect},
	pages = {653--662}
},

@article{bordes_fast_2005,
	title = {Fast Kernel Classifiers with Online and Active Learning},
	volume = {6},
	url = {http://portal.acm.org/citation.cfm?id=1046920.1194898&coll=ACM&dl=ACM&CFID=96681423&CFTOKEN=43711541},
	abstract = {Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efficient learning algorithm should at least take a brief look at each example. But should all examples be given equal {attention?This} contribution proposes an empirical answer. We first present an online {SVM} algorithm based on this premise. {LASVM} yields competitive misclassification rates after a single pass over the training examples, outspeeding state-of-the-art {SVM} solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.},
	journal = {J. Mach. Learn. Res.},
	author = {Antoine Bordes and Seyda Ertekin and Jason Weston and L\'{e}on Bottou},
	year = {2005},
	pages = {1579--1619}
},

@book{russell_artificial_2004,
	title = {Artificial Intelligence: a Modern Approach},
	isbn = {9780137903023},
	shorttitle = {Artificial Intelligence},
	publisher = {Pearson plc},
	author = {Stuart Russell},
	month = dec,
	year = {2004}
},

@article{kramer_fast_2009,
	title = {Fast Support Vector Machines for Continuous Data},
	volume = {39},
	issn = {1083-4419},
	doi = {10.1109/TSMCB.2008.2011645},
	abstract = {Support vector machines {(SVMs)} can be trained to be very accurate classifiers and have been used in many applications. However, the training time and, to a lesser extent, prediction time of {SVMs} on very large data sets can be very long. This paper presents a fast compression method to scale up {SVMs} to large data sets. A simple bit-reduction method is applied to reduce the cardinality of the data by weighting representative examples. We then develop {SVMs} trained on the weighted data. Experiments indicate that bit-reduction {SVM} produces a significant reduction in the time required for both training and prediction with minimum loss in accuracy. It is also shown to typically be more accurate than random sampling when the data are not overcompressed.},
	number = {4},
	journal = {Systems, Man, and Cybernetics, Part B: Cybernetics, {IEEE} Transactions on},
	author = {{K.A.} Kramer and {L.O.} Hall and {D.B.} Goldgof and A. Remsen and Tong Luo},
	year = {2009},
	keywords = {data compression, data representation, data structures, fast compression method, fast support vector machines, large data sets, simple bit-reduction method, support vector machines},
	pages = {989--1001}
},

@misc{_multiclass_????,
	title = {Multiclass {SVMs}},
	url = {http://nlp.stanford.edu/IR-book/html/htmledition/multiclass-svms-1.html},
	howpublished = {{http://nlp.stanford.edu/IR-book/html/htmledition/multiclass-svms-1.html}}
},

@incollection{belazzougui_hash_2009,
	title = {Hash, Displace, and Compress},
	url = {http://dx.doi.org/10.1007/978-3-642-04128-0_61},
	abstract = {A hash function h, i.e., a function from the set U of all keys to the range range [m] = {0,...,m - 1} is called a perfect hash function {(PHF)} for a subset S ? U of size n <= m if h is 1-1 on S. The important performance parameters of a {PHF} are representation size, evaluation time and construction time. In this paper,
we present an algorithm that permits to obtain {PHFs} with expected representation size very close to optimal while retaining
O(n) expected construction time and O(1) evaluation time in the worst case. For example in the case m = 1.23n we obtain a {PHF} that uses space 1.4 bits per key, and for m = 1.01n we obtain space 1.98 bits per key, which was not achievable with previously known methods. Our algorithm is inspired by several
known algorithms; the main new feature is that we combine a modification of Pagh{\textquoteright}s {\textquotedblleft}hash-and-displace{\textquotedblright} approach with data
compression on a sequence of hash function indices. Our algorithm can also be used for k-perfect hashing, where at most k keys may be mapped to the same value.},
	booktitle = {Algorithms - {ESA} 2009},
	author = {Djamal Belazzougui and Fabiano Botelho and Martin Dietzfelbinger},
	year = {2009},
	pages = {682--693}
},

@book{alpaydin_introduction_2004,
	title = {Introduction to machine learning},
	isbn = {9780262012119},
	publisher = {{MIT} Press},
	author = {Ethem Alpaydin},
	month = oct,
	year = {2004}
},

@misc{_smoothing_????,
	title = {Smoothing a tera-word language model},
	url = {http://portal.acm.org/citation.cfm?id=1557727&dl=},
	howpublished = {http://portal.acm.org/citation.cfm?id=1557727\&dl=}
},

@inproceedings{crammer_advanced_2008,
	address = {Columbus, Ohio},
	title = {Advanced online learning for natural language processing},
	url = {http://portal.acm.org/citation.cfm?id=1564169.1564173&coll=ACM&dl=ACM&CFID=96681423&CFTOKEN=43711541},
	abstract = {Most research in machine learning has been focused on binary classification, in which the learned classifier outputs one of two possible answers. Important fundamental questions can be analyzed in terms of binary classification, but real-world natural language processing problems often involve richer output spaces. In this tutorial, we will focus on classifiers with a large number of possible outputs with interesting structure. Notable examples include information retrieval, part-of-speech tagging, {NP} chucking, parsing, entity extraction, and phoneme recognition.},
	booktitle = {Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Tutorial Abstracts},
	publisher = {Association for Computational Linguistics},
	author = {Koby Crammer},
	year = {2008},
	pages = {4--4}
},

@book{love_attributing_2002,
	title = {Attributing authorship: an introduction},
	isbn = {9780521789486},
	shorttitle = {Attributing authorship},
	publisher = {Cambridge University Press},
	author = {Harold Love},
	month = jun,
	year = {2002}
}